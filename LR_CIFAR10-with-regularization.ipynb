{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LR-CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EptlwQwxWKkU",
        "outputId": "734ae7c8-b13d-4b52-aee0-956d0217ce9e"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# define CIFAR10 Dataset and load\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "batch_size = 4\n",
        "Train_Dataset = torchvision.datasets.CIFAR10(root = './data',train = True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "Test_Dataset = torchvision.datasets.CIFAR10(root = './data', train = False,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "Train_loader = torch.utils.data.DataLoader(Train_Dataset, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "Test_loader = torch.utils.data.DataLoader(Test_Dataset, batch_size = batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "classes= ('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXB4tmwqY0EM"
      },
      "source": [
        "#Define Convolutional Neural Network\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3,6,5)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(6,16,5)\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc2 = nn.Linear(120,84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x,1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x= self.fc3(x)\n",
        "    return x\n",
        "\n",
        "net = Net()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRUej5dlaTf1"
      },
      "source": [
        "#Lost Function and Optimizer\n",
        "import torch.optim as optim\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "weight = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# with regularization\n",
        "optimizer = optim.SGD(net.parameters(),lr=learning_rate,momentum=momentum,weight_decay=weight)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y76_eJ4atvC",
        "outputId": "fc9f8f0e-f8f9-4dcf-d6a4-073fb359d0b8"
      },
      "source": [
        "#Training\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(Train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "for epoch in range(2):\n",
        "  loss = 0.0\n",
        "  for i,data in enumerate(Train_loader,0):\n",
        "    # get the inputs\n",
        "    inputs,labels = data\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss_func = criterion(outputs,labels)\n",
        "    loss_func.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss +=loss_func.item()\n",
        "    if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %\n",
        "      (epoch + 1, i + 1, loss / 2000))\n",
        "      loss = 0.0\n",
        "\n",
        "\n",
        "print('Training done !')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  1000] loss: 0.698\n",
            "[1,  2000] loss: 0.690\n",
            "[1,  3000] loss: 0.690\n",
            "[1,  4000] loss: 0.692\n",
            "[1,  5000] loss: 0.684\n",
            "[1,  6000] loss: 0.681\n",
            "[1,  7000] loss: 0.705\n",
            "[1,  8000] loss: 0.698\n",
            "[1,  9000] loss: 0.685\n",
            "[1, 10000] loss: 0.693\n",
            "[1, 11000] loss: 0.685\n",
            "[1, 12000] loss: 0.669\n",
            "[2,  1000] loss: 0.663\n",
            "[2,  2000] loss: 0.665\n",
            "[2,  3000] loss: 0.667\n",
            "[2,  4000] loss: 0.674\n",
            "[2,  5000] loss: 0.651\n",
            "[2,  6000] loss: 0.661\n",
            "[2,  7000] loss: 0.656\n",
            "[2,  8000] loss: 0.665\n",
            "[2,  9000] loss: 0.658\n",
            "[2, 10000] loss: 0.673\n",
            "[2, 11000] loss: 0.666\n",
            "[2, 12000] loss: 0.656\n",
            "Training done !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeEg1NcolJUE",
        "outputId": "ab52369a-7ef3-40ce-bfad-9872a0b50778"
      },
      "source": [
        "#Accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in Test_loader:\n",
        "    images,labels = data\n",
        "    outputs = net(images)\n",
        "\n",
        "    _,predicted = torch.max(outputs.data,1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "accuracy = 100*(correct.item()/ total)\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images:',accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 54.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFs0nDo3nFin",
        "outputId": "8f184817-e2ab-4ad3-e867-eb6064b6e06b"
      },
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_class = {classname: 0 for classname in classes}\n",
        "total_class = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in Test_loader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_class[classes[label]] += 1\n",
        "            total_class[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_class.items():\n",
        "    accuracy_classes = 100 * float(correct_count) / total_class[classname]\n",
        "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
        "                                                   accuracy_classes))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class plane is: 46.3 %\n",
            "Accuracy for class car   is: 64.4 %\n",
            "Accuracy for class bird  is: 58.4 %\n",
            "Accuracy for class cat   is: 42.3 %\n",
            "Accuracy for class deer  is: 49.0 %\n",
            "Accuracy for class dog   is: 34.7 %\n",
            "Accuracy for class frog  is: 69.4 %\n",
            "Accuracy for class horse is: 52.2 %\n",
            "Accuracy for class ship  is: 65.2 %\n",
            "Accuracy for class truck is: 60.6 %\n"
          ]
        }
      ]
    }
  ]
}