{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**HW 5 - LSTM**"
      ],
      "metadata": {
        "id": "YNbDMvKcpXD7"
      },
      "id": "YNbDMvKcpXD7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating Melodies"
      ],
      "metadata": {
        "id": "CkndKtavvMfZ"
      },
      "id": "CkndKtavvMfZ"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "26ba911a",
      "metadata": {
        "id": "26ba911a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import music21 as m21\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "warnings.filterwarnings(\"warning\")\n",
        "from fractions import Fraction\n",
        "from typing import Dict, List, Optional, Sequence, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect Drive"
      ],
      "metadata": {
        "id": "X3cXfgkDwJ-K"
      },
      "id": "X3cXfgkDwJ-K"
    },
    {
      "cell_type": "code",
      "source": [
        "#getting from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRBgesphol7d",
        "outputId": "aaf2e86a-b176-424b-ca9f-e0f1f0e2da95"
      },
      "id": "XRBgesphol7d",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Path"
      ],
      "metadata": {
        "id": "gm6JTLiVwMbJ"
      },
      "id": "gm6JTLiVwMbJ"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0caf6247",
      "metadata": {
        "id": "0caf6247"
      },
      "outputs": [],
      "source": [
        "music_path = r\"/content/drive/MyDrive/midi\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Music Durations"
      ],
      "metadata": {
        "id": "e4f2yHz8wQnC"
      },
      "id": "e4f2yHz8wQnC"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "47502fd7",
      "metadata": {
        "id": "47502fd7"
      },
      "outputs": [],
      "source": [
        "music_duration=[0.25,\n",
        "                0.55,\n",
        "                0.75,\n",
        "                1.3,\n",
        "                1.5,\n",
        "                2.75,\n",
        "                3.,\n",
        "                4.]\n",
        "\n",
        "#ayarlanan music dosyasının uzantısını algılayabilmesi için\n",
        "# using music file = .midi extension\n",
        "def load(music_path):\n",
        "    musics = []\n",
        "    for process in os.listdir(music_path):\n",
        "        warnings.filterwarnings(\"warning\")\n",
        "        if  process[-3:] == \"mid\":\n",
        "            musics.append(m21.converter.parse((music_path+\"\\{}\").format(process)))\n",
        "    return musics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a3b91c9f",
      "metadata": {
        "id": "a3b91c9f"
      },
      "outputs": [],
      "source": [
        "def data(musics):\n",
        "    x = []\n",
        "    size= []\n",
        "    for process in musics:\n",
        "        for b in process:\n",
        "            notes = [a for a in b.flat.notesAndRests]\n",
        "            b = 0\n",
        "            for i in notes:\n",
        "                b = b+1\n",
        "                if isinstance(i,m21.note.Note):\n",
        "                    x.append([i.pitch.midi,float(i.m_duration.quarterLength)])\n",
        "                else:\n",
        "                    x.append([0,float(i.m_duration.quarterLength)])\n",
        "            size.append(batch)\n",
        "    dataset = pd.DataFrame(x,columns=[\"First column\", \"Second column\"])\n",
        "    return dataset,size\n",
        "\n",
        "# major ve minor of musical notes \n",
        "def transpose(process):\n",
        "    notas = process.analyze(\"notas\")\n",
        "    if notas.mode == \"minor\":\n",
        "        interval = m21.interval.Interval(notas.tonic,m21.pitch.Pitch(\"A\"))\n",
        "    if notas.mode == \"major\":\n",
        "        interval = m21.interval.Interval(notas.tonic,m21.pitch.Pitch(\"C\"))\n",
        "    \n",
        "    return process.transpose(interval)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8ad909d5",
      "metadata": {
        "id": "8ad909d5"
      },
      "outputs": [],
      "source": [
        "def m_duration(process,music_duration):\n",
        "    for note in  process.flat.notesAndRests:\n",
        "        if note.m_duration.quarterLength not in music_duration:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def preprocess(music_path,music_duration):\n",
        "    musics = load(music_path)\n",
        "    b = []\n",
        "    for process in musics:\n",
        "        if not m_duration(process,music_duration):\n",
        "            continue\n",
        "        b.append(transpose(process))\n",
        "    \n",
        "    return b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0157bad3",
      "metadata": {
        "id": "0157bad3"
      },
      "outputs": [],
      "source": [
        "def transform(model,data):\n",
        "        for d in data:\n",
        "            b = []\n",
        "            b.append(model.ohe.transform(d).toarray())\n",
        "        return np.array(b)\n",
        "\n",
        "# one hot encoding\n",
        "\n",
        "def batch(encoded,samp_per_batch=12, sequation_lenght = 10):\n",
        "    note_per_batch = samp_per_batch * sequation_lenght\n",
        "    num_batches = int(len(encoded)/note_per_batch)\n",
        "    encoded = encoded[:num_batches * note_per_batch]\n",
        "    state = True\n",
        "    for n in range(0, (encoded.shape[0]-sequation_lenght), sequation_lenght):\n",
        "        if state:\n",
        "            x = np.array([encoded[n:n+sequation_lenght]])\n",
        "            y = np.array([encoded[n+1:n+sequation_lenght+1]])\n",
        "            state = False\n",
        "            n2 = 0\n",
        "        else:\n",
        "            n2 +=1\n",
        "            x = np.append(x,np.array([encoded[n:n+sequation_lenght]]),axis=0)\n",
        "            try:\n",
        "                y= np.append(y,np.array([encoded[n+1:n+sequation_lenght+1]]),axis=0)\n",
        "            except:\n",
        "                y= np.append(y,np.array([encoded[n+1:n+sequation_lenght]]),axis=0)\n",
        "                y= np.append(y,np.array([[encoded[0]]]),axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb5741c",
      "metadata": {
        "id": "cdb5741c"
      },
      "outputs": [],
      "source": [
        "def output(model,arr):\n",
        "    model.eval()\n",
        "    arr = transform(model,np.array([arr]))\n",
        "    hidden = model.hidden_state(1)\n",
        "    inputs = torch.from_numpy(arr).float()\n",
        "    inputs = inputs.cuda()\n",
        "    lstm_output, hidden = model.forward(inputs,hidden)\n",
        "    lstm_output,_ = model(inputs,hidden)\n",
        "    q = lstm_output.cpu().detach().clone().numpy()\n",
        "    return model.ohe.inverse_transform(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3653c8a6",
      "metadata": {
        "id": "3653c8a6"
      },
      "outputs": [],
      "source": [
        "process = preprocess(music_path,music_duration)\n",
        "dataset = data(process)\n",
        "b = data(process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd91259b",
      "metadata": {
        "id": "bd91259b"
      },
      "outputs": [],
      "source": [
        "# Setting Neural Network (LSTM)\n",
        "\n",
        "class Model_(nn.Module):\n",
        "    \n",
        "    def __init__(self, dataset, num_hidden, num_layers, drop, use_gpu):\n",
        "    #setting class\n",
        "        num_hidden=256\n",
        "        drop=0.5\n",
        "        num_layers=4\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        #Setting Encoder       \n",
        "        self.dataset = dataset\n",
        "        self.ohe = OneHotEncoder(sparse=True)\n",
        "        self.ohe.fit_transform(dataset)\n",
        "        self.uni = len(self.ohe.get_feature_names())\n",
        "             \n",
        "        #using LSTM (from research)\n",
        "        self.lstm = nn.LSTM(self.uni, num_hidden, num_layers, dropout=drop,batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop)\n",
        "        \n",
        "        self.fc_linear = nn.Linear(num_hidden, self.uni)\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "                        \n",
        "        lstm_output, hidden = self.lstm(x, hidden)      \n",
        "        drop_output = self.dropout(lstm_output)       \n",
        "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
        "        \n",
        "        \n",
        "        final_out = self.fc_linear(drop_output)        \n",
        "        return final_out, hidden\n",
        "    \n",
        "    \n",
        "    def hidden_state(self, batch_size):\n",
        "        '''\n",
        "        Used as separate method to account for both GPU and CPU users.\n",
        "        '''\n",
        "        \n",
        "        if self.use_gpu:\n",
        "            \n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "        else:\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "        \n",
        "        return hidden\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9973f5aa",
      "metadata": {
        "id": "9973f5aa"
      },
      "outputs": [],
      "source": [
        "model = Model_(dataset=dataset, num_hidden=50, num_layers=2, drop_prob=0.5, use_gpu=True)\n",
        "\n",
        "#using Adam\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_dataset= dataset.values\n",
        "test_dataset= dataset.values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "-E6JgZiQw-sL"
      },
      "id": "-E6JgZiQw-sL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600bffc8",
      "metadata": {
        "id": "600bffc8",
        "outputId": "d41097c6-faf7-477b-cb77-3875b9aea367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 Step: 25 Val Loss: 4.918031692504883\n",
            "Epoch: 0 Step: 50 Val Loss: 3.9465954303741455\n",
            "Epoch: 0 Step: 75 Val Loss: 3.764733076095581\n",
            "Epoch: 0 Step: 100 Val Loss: 3.708934783935547\n",
            "Epoch: 1 Step: 125 Val Loss: 3.668837785720825\n",
            "Epoch: 1 Step: 150 Val Loss: 3.7347631454467773\n",
            "Epoch: 1 Step: 175 Val Loss: 3.702684164047241\n",
            "Epoch: 1 Step: 200 Val Loss: 3.764221429824829\n",
            "Epoch: 2 Step: 225 Val Loss: 3.604902505874634\n",
            "Epoch: 2 Step: 250 Val Loss: 3.67340087890625\n",
            "Epoch: 2 Step: 275 Val Loss: 3.59416127204895\n",
            "Epoch: 2 Step: 300 Val Loss: 3.61981463432312\n",
            "Epoch: 3 Step: 325 Val Loss: 3.5099918842315674\n",
            "Epoch: 3 Step: 350 Val Loss: 3.4956114292144775\n",
            "Epoch: 3 Step: 375 Val Loss: 3.5660207271575928\n",
            "Epoch: 3 Step: 400 Val Loss: 3.585008382797241\n",
            "Epoch: 3 Step: 425 Val Loss: 3.5726890563964844\n",
            "Epoch: 4 Step: 450 Val Loss: 3.430044412612915\n",
            "Epoch: 4 Step: 475 Val Loss: 3.4965248107910156\n",
            "Epoch: 4 Step: 500 Val Loss: 3.541926860809326\n",
            "Epoch: 4 Step: 525 Val Loss: 3.618281126022339\n",
            "Epoch: 5 Step: 550 Val Loss: 3.408087730407715\n",
            "Epoch: 5 Step: 575 Val Loss: 3.4739599227905273\n",
            "Epoch: 5 Step: 600 Val Loss: 3.5337297916412354\n",
            "Epoch: 5 Step: 625 Val Loss: 3.5937535762786865\n",
            "Epoch: 6 Step: 650 Val Loss: 3.4612138271331787\n",
            "Epoch: 6 Step: 675 Val Loss: 3.432081460952759\n",
            "Epoch: 6 Step: 700 Val Loss: 3.518710136413574\n",
            "Epoch: 6 Step: 725 Val Loss: 3.5437257289886475\n",
            "Epoch: 6 Step: 750 Val Loss: 3.513739585876465\n",
            "Epoch: 7 Step: 775 Val Loss: 3.3742566108703613\n",
            "Epoch: 7 Step: 800 Val Loss: 3.4418880939483643\n",
            "Epoch: 7 Step: 825 Val Loss: 3.49170184135437\n",
            "Epoch: 7 Step: 850 Val Loss: 3.576469898223877\n",
            "Epoch: 8 Step: 875 Val Loss: 3.323970317840576\n",
            "Epoch: 8 Step: 900 Val Loss: 3.3614213466644287\n",
            "Epoch: 8 Step: 925 Val Loss: 3.466364622116089\n",
            "Epoch: 8 Step: 950 Val Loss: 3.5069613456726074\n",
            "Epoch: 9 Step: 975 Val Loss: 3.290511131286621\n",
            "Epoch: 9 Step: 1000 Val Loss: 3.3206591606140137\n",
            "Epoch: 9 Step: 1025 Val Loss: 3.347496271133423\n",
            "Epoch: 9 Step: 1050 Val Loss: 3.337174654006958\n",
            "Epoch: 9 Step: 1075 Val Loss: 3.2826461791992188\n",
            "Epoch: 10 Step: 1100 Val Loss: 3.12780499458313\n",
            "Epoch: 10 Step: 1125 Val Loss: 3.1856143474578857\n",
            "Epoch: 10 Step: 1150 Val Loss: 3.2158544063568115\n",
            "Epoch: 10 Step: 1175 Val Loss: 3.238542318344116\n",
            "Epoch: 11 Step: 1200 Val Loss: 3.0394794940948486\n",
            "Epoch: 11 Step: 1225 Val Loss: 3.100712776184082\n",
            "Epoch: 11 Step: 1250 Val Loss: 3.172996997833252\n",
            "Epoch: 11 Step: 1275 Val Loss: 3.1588375568389893\n",
            "Epoch: 12 Step: 1300 Val Loss: 3.0186054706573486\n",
            "Epoch: 12 Step: 1325 Val Loss: 3.0244264602661133\n",
            "Epoch: 12 Step: 1350 Val Loss: 3.142453193664551\n",
            "Epoch: 12 Step: 1375 Val Loss: 3.081787586212158\n",
            "Epoch: 12 Step: 1400 Val Loss: 3.029489278793335\n",
            "Epoch: 13 Step: 1425 Val Loss: 2.93416690826416\n",
            "Epoch: 13 Step: 1450 Val Loss: 3.049046039581299\n",
            "Epoch: 13 Step: 1475 Val Loss: 3.04119873046875\n",
            "Epoch: 13 Step: 1500 Val Loss: 3.031643867492676\n",
            "Epoch: 14 Step: 1525 Val Loss: 2.887316942214966\n",
            "Epoch: 14 Step: 1550 Val Loss: 2.960322141647339\n",
            "Epoch: 14 Step: 1575 Val Loss: 3.076427936553955\n",
            "Epoch: 14 Step: 1600 Val Loss: 3.0112342834472656\n",
            "Epoch: 15 Step: 1625 Val Loss: 2.87845516204834\n",
            "Epoch: 15 Step: 1650 Val Loss: 2.892439603805542\n",
            "Epoch: 15 Step: 1675 Val Loss: 2.9894909858703613\n",
            "Epoch: 15 Step: 1700 Val Loss: 3.0020995140075684\n",
            "Epoch: 15 Step: 1725 Val Loss: 2.9054412841796875\n",
            "Epoch: 16 Step: 1750 Val Loss: 2.80409574508667\n",
            "Epoch: 16 Step: 1775 Val Loss: 2.881990909576416\n",
            "Epoch: 16 Step: 1800 Val Loss: 2.925889015197754\n",
            "Epoch: 16 Step: 1825 Val Loss: 2.8893768787384033\n",
            "Epoch: 17 Step: 1850 Val Loss: 2.7318782806396484\n",
            "Epoch: 17 Step: 1875 Val Loss: 2.8265459537506104\n",
            "Epoch: 17 Step: 1900 Val Loss: 2.970245838165283\n",
            "Epoch: 17 Step: 1925 Val Loss: 2.865424394607544\n",
            "Epoch: 18 Step: 1950 Val Loss: 2.7283501625061035\n",
            "Epoch: 18 Step: 1975 Val Loss: 2.7511937618255615\n",
            "Epoch: 18 Step: 2000 Val Loss: 2.871055841445923\n",
            "Epoch: 18 Step: 2025 Val Loss: 2.821936845779419\n",
            "Epoch: 18 Step: 2050 Val Loss: 2.7434771060943604\n",
            "Epoch: 19 Step: 2075 Val Loss: 2.6449475288391113\n",
            "Epoch: 19 Step: 2100 Val Loss: 2.795421600341797\n",
            "Epoch: 19 Step: 2125 Val Loss: 2.774338960647583\n",
            "Epoch: 19 Step: 2150 Val Loss: 2.731005907058716\n",
            "Epoch: 20 Step: 2175 Val Loss: 2.5533666610717773\n",
            "Epoch: 20 Step: 2200 Val Loss: 2.709918260574341\n",
            "Epoch: 20 Step: 2225 Val Loss: 2.7530605792999268\n",
            "Epoch: 20 Step: 2250 Val Loss: 2.7864861488342285\n",
            "Epoch: 21 Step: 2275 Val Loss: 2.5297839641571045\n",
            "Epoch: 21 Step: 2300 Val Loss: 2.603294849395752\n",
            "Epoch: 21 Step: 2325 Val Loss: 2.7068939208984375\n",
            "Epoch: 21 Step: 2350 Val Loss: 2.6695990562438965\n",
            "Epoch: 21 Step: 2375 Val Loss: 2.53963041305542\n",
            "Epoch: 22 Step: 2400 Val Loss: 2.481541872024536\n",
            "Epoch: 22 Step: 2425 Val Loss: 2.5664074420928955\n",
            "Epoch: 22 Step: 2450 Val Loss: 2.63206148147583\n",
            "Epoch: 22 Step: 2475 Val Loss: 2.5445120334625244\n",
            "Epoch: 23 Step: 2500 Val Loss: 2.378107786178589\n",
            "Epoch: 23 Step: 2525 Val Loss: 2.5026025772094727\n",
            "Epoch: 23 Step: 2550 Val Loss: 2.56986665725708\n",
            "Epoch: 23 Step: 2575 Val Loss: 2.686993360519409\n",
            "Epoch: 24 Step: 2600 Val Loss: 2.334285259246826\n",
            "Epoch: 24 Step: 2625 Val Loss: 2.465757369995117\n",
            "Epoch: 24 Step: 2650 Val Loss: 2.5699281692504883\n",
            "Epoch: 24 Step: 2675 Val Loss: 2.567277431488037\n",
            "Epoch: 24 Step: 2700 Val Loss: 2.3647825717926025\n",
            "Epoch: 25 Step: 2725 Val Loss: 2.37536883354187\n",
            "Epoch: 25 Step: 2750 Val Loss: 2.3803279399871826\n",
            "Epoch: 25 Step: 2775 Val Loss: 2.4405181407928467\n",
            "Epoch: 25 Step: 2800 Val Loss: 2.3561837673187256\n",
            "Epoch: 26 Step: 2825 Val Loss: 2.265284299850464\n",
            "Epoch: 26 Step: 2850 Val Loss: 2.3321213722229004\n",
            "Epoch: 26 Step: 2875 Val Loss: 2.418579339981079\n",
            "Epoch: 26 Step: 2900 Val Loss: 2.581876277923584\n",
            "Epoch: 27 Step: 2925 Val Loss: 2.2250564098358154\n",
            "Epoch: 27 Step: 2950 Val Loss: 2.316781520843506\n",
            "Epoch: 27 Step: 2975 Val Loss: 2.453839063644409\n",
            "Epoch: 27 Step: 3000 Val Loss: 2.456077814102173\n",
            "Epoch: 28 Step: 3025 Val Loss: 2.2253994941711426\n",
            "Epoch: 28 Step: 3050 Val Loss: 2.2138965129852295\n",
            "Epoch: 28 Step: 3075 Val Loss: 2.2848873138427734\n",
            "Epoch: 28 Step: 3100 Val Loss: 2.2812232971191406\n",
            "Epoch: 28 Step: 3125 Val Loss: 2.2308804988861084\n",
            "Epoch: 29 Step: 3150 Val Loss: 2.1516788005828857\n",
            "Epoch: 29 Step: 3175 Val Loss: 2.210421562194824\n",
            "Epoch: 29 Step: 3200 Val Loss: 2.2646467685699463\n",
            "Epoch: 29 Step: 3225 Val Loss: 2.3907909393310547\n",
            "Epoch: 30 Step: 3250 Val Loss: 2.124627113342285\n",
            "Epoch: 30 Step: 3275 Val Loss: 2.1590418815612793\n",
            "Epoch: 30 Step: 3300 Val Loss: 2.3780264854431152\n",
            "Epoch: 30 Step: 3325 Val Loss: 2.2703981399536133\n",
            "Epoch: 31 Step: 3350 Val Loss: 2.115398406982422\n",
            "Epoch: 31 Step: 3375 Val Loss: 2.097306966781616\n",
            "Epoch: 31 Step: 3400 Val Loss: 2.192866563796997\n",
            "Epoch: 31 Step: 3425 Val Loss: 2.1470088958740234\n",
            "Epoch: 31 Step: 3450 Val Loss: 2.124886989593506\n",
            "Epoch: 32 Step: 3475 Val Loss: 2.05112361907959\n",
            "Epoch: 32 Step: 3500 Val Loss: 2.095106363296509\n",
            "Epoch: 32 Step: 3525 Val Loss: 2.119011878967285\n",
            "Epoch: 32 Step: 3550 Val Loss: 2.2134459018707275\n",
            "Epoch: 33 Step: 3575 Val Loss: 2.0281589031219482\n",
            "Epoch: 33 Step: 3600 Val Loss: 2.052138090133667\n",
            "Epoch: 33 Step: 3625 Val Loss: 2.2272603511810303\n",
            "Epoch: 33 Step: 3650 Val Loss: 2.135378122329712\n",
            "Epoch: 34 Step: 3675 Val Loss: 2.0346102714538574\n",
            "Epoch: 34 Step: 3700 Val Loss: 2.0134494304656982\n",
            "Epoch: 34 Step: 3725 Val Loss: 2.110975503921509\n",
            "Epoch: 34 Step: 3750 Val Loss: 2.080420970916748\n",
            "Epoch: 34 Step: 3775 Val Loss: 2.0553483963012695\n",
            "Epoch: 35 Step: 3800 Val Loss: 1.979467749595642\n",
            "Epoch: 35 Step: 3825 Val Loss: 2.054894208908081\n",
            "Epoch: 35 Step: 3850 Val Loss: 2.0500824451446533\n",
            "Epoch: 35 Step: 3875 Val Loss: 2.0789546966552734\n",
            "Epoch: 36 Step: 3900 Val Loss: 1.9591333866119385\n",
            "Epoch: 36 Step: 3925 Val Loss: 1.9828509092330933\n",
            "Epoch: 36 Step: 3950 Val Loss: 2.0912258625030518\n",
            "Epoch: 36 Step: 3975 Val Loss: 2.034386396408081\n",
            "Epoch: 37 Step: 4000 Val Loss: 1.9591519832611084\n",
            "Epoch: 37 Step: 4025 Val Loss: 1.9431673288345337\n",
            "Epoch: 37 Step: 4050 Val Loss: 2.0301835536956787\n",
            "Epoch: 37 Step: 4075 Val Loss: 1.9963783025741577\n",
            "Epoch: 37 Step: 4100 Val Loss: 1.968711018562317\n",
            "Epoch: 38 Step: 4125 Val Loss: 1.9324086904525757\n",
            "Epoch: 38 Step: 4150 Val Loss: 1.9777642488479614\n",
            "Epoch: 38 Step: 4175 Val Loss: 1.9805818796157837\n",
            "Epoch: 38 Step: 4200 Val Loss: 2.017820119857788\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 39 Step: 4225 Val Loss: 1.9103710651397705\n",
            "Epoch: 39 Step: 4250 Val Loss: 1.922908902168274\n",
            "Epoch: 39 Step: 4275 Val Loss: 2.01640248298645\n",
            "Epoch: 39 Step: 4300 Val Loss: 1.964058756828308\n",
            "Epoch: 40 Step: 4325 Val Loss: 1.9069417715072632\n",
            "Epoch: 40 Step: 4350 Val Loss: 1.8944100141525269\n",
            "Epoch: 40 Step: 4375 Val Loss: 1.958102822303772\n",
            "Epoch: 40 Step: 4400 Val Loss: 1.9306107759475708\n",
            "Epoch: 40 Step: 4425 Val Loss: 1.9208053350448608\n",
            "Epoch: 41 Step: 4450 Val Loss: 1.86720871925354\n",
            "Epoch: 41 Step: 4475 Val Loss: 1.8928847312927246\n",
            "Epoch: 41 Step: 4500 Val Loss: 1.9044039249420166\n",
            "Epoch: 41 Step: 4525 Val Loss: 1.9232709407806396\n",
            "Epoch: 42 Step: 4550 Val Loss: 1.8599663972854614\n",
            "Epoch: 42 Step: 4575 Val Loss: 1.8798044919967651\n",
            "Epoch: 42 Step: 4600 Val Loss: 1.926985263824463\n",
            "Epoch: 42 Step: 4625 Val Loss: 1.911041498184204\n",
            "Epoch: 43 Step: 4650 Val Loss: 1.8501887321472168\n",
            "Epoch: 43 Step: 4675 Val Loss: 1.84364652633667\n",
            "Epoch: 43 Step: 4700 Val Loss: 1.9022005796432495\n",
            "Epoch: 43 Step: 4725 Val Loss: 1.8704851865768433\n",
            "Epoch: 43 Step: 4750 Val Loss: 1.8653910160064697\n",
            "Epoch: 44 Step: 4775 Val Loss: 1.8377599716186523\n",
            "Epoch: 44 Step: 4800 Val Loss: 1.858012080192566\n",
            "Epoch: 44 Step: 4825 Val Loss: 1.8575663566589355\n",
            "Epoch: 44 Step: 4850 Val Loss: 1.8636432886123657\n",
            "Epoch: 45 Step: 4875 Val Loss: 1.8186266422271729\n",
            "Epoch: 45 Step: 4900 Val Loss: 1.8356159925460815\n",
            "Epoch: 45 Step: 4925 Val Loss: 1.8715182542800903\n",
            "Epoch: 45 Step: 4950 Val Loss: 1.8721086978912354\n",
            "Epoch: 46 Step: 4975 Val Loss: 1.815260887145996\n",
            "Epoch: 46 Step: 5000 Val Loss: 1.799919605255127\n",
            "Epoch: 46 Step: 5025 Val Loss: 1.850866675376892\n",
            "Epoch: 46 Step: 5050 Val Loss: 1.8450664281845093\n",
            "Epoch: 46 Step: 5075 Val Loss: 1.8232375383377075\n",
            "Epoch: 47 Step: 5100 Val Loss: 1.7972558736801147\n",
            "Epoch: 47 Step: 5125 Val Loss: 1.827175498008728\n",
            "Epoch: 47 Step: 5150 Val Loss: 1.8184146881103516\n",
            "Epoch: 47 Step: 5175 Val Loss: 1.8178009986877441\n",
            "Epoch: 48 Step: 5200 Val Loss: 1.7911009788513184\n",
            "Epoch: 48 Step: 5225 Val Loss: 1.8104363679885864\n",
            "Epoch: 48 Step: 5250 Val Loss: 1.8398566246032715\n",
            "Epoch: 48 Step: 5275 Val Loss: 1.8549079895019531\n",
            "Epoch: 49 Step: 5300 Val Loss: 1.7793772220611572\n",
            "Epoch: 49 Step: 5325 Val Loss: 1.7780519723892212\n",
            "Epoch: 49 Step: 5350 Val Loss: 1.838112711906433\n",
            "Epoch: 49 Step: 5375 Val Loss: 1.8048886060714722\n",
            "Epoch: 49 Step: 5400 Val Loss: 1.7772250175476074\n",
            "Epoch: 50 Step: 5425 Val Loss: 1.793310523033142\n",
            "Epoch: 50 Step: 5450 Val Loss: 1.8145335912704468\n",
            "Epoch: 50 Step: 5475 Val Loss: 1.798964500427246\n",
            "Epoch: 50 Step: 5500 Val Loss: 1.7864298820495605\n",
            "Epoch: 51 Step: 5525 Val Loss: 1.747194528579712\n",
            "Epoch: 51 Step: 5550 Val Loss: 1.7642275094985962\n",
            "Epoch: 51 Step: 5575 Val Loss: 1.784143328666687\n",
            "Epoch: 51 Step: 5600 Val Loss: 1.81203031539917\n",
            "Epoch: 52 Step: 5625 Val Loss: 1.7448700666427612\n",
            "Epoch: 52 Step: 5650 Val Loss: 1.7614758014678955\n",
            "Epoch: 52 Step: 5675 Val Loss: 1.7893372774124146\n",
            "Epoch: 52 Step: 5700 Val Loss: 1.78225576877594\n",
            "Epoch: 53 Step: 5725 Val Loss: 1.7460830211639404\n",
            "Epoch: 53 Step: 5750 Val Loss: 1.719844937324524\n",
            "Epoch: 53 Step: 5775 Val Loss: 1.7769277095794678\n",
            "Epoch: 53 Step: 5800 Val Loss: 1.7604247331619263\n",
            "Epoch: 53 Step: 5825 Val Loss: 1.7441920042037964\n",
            "Epoch: 54 Step: 5850 Val Loss: 1.7077977657318115\n",
            "Epoch: 54 Step: 5875 Val Loss: 1.729996919631958\n",
            "Epoch: 54 Step: 5900 Val Loss: 1.7380911111831665\n",
            "Epoch: 54 Step: 5925 Val Loss: 1.754152774810791\n",
            "Epoch: 55 Step: 5950 Val Loss: 1.7061454057693481\n",
            "Epoch: 55 Step: 5975 Val Loss: 1.708820104598999\n",
            "Epoch: 55 Step: 6000 Val Loss: 1.7457307577133179\n",
            "Epoch: 55 Step: 6025 Val Loss: 1.7454273700714111\n",
            "Epoch: 56 Step: 6050 Val Loss: 1.7144145965576172\n",
            "Epoch: 56 Step: 6075 Val Loss: 1.700785756111145\n",
            "Epoch: 56 Step: 6100 Val Loss: 1.7340387105941772\n",
            "Epoch: 56 Step: 6125 Val Loss: 1.7341493368148804\n",
            "Epoch: 56 Step: 6150 Val Loss: 1.722569227218628\n",
            "Epoch: 57 Step: 6175 Val Loss: 1.688276767730713\n",
            "Epoch: 57 Step: 6200 Val Loss: 1.7078509330749512\n",
            "Epoch: 57 Step: 6225 Val Loss: 1.7093762159347534\n",
            "Epoch: 57 Step: 6250 Val Loss: 1.7288968563079834\n",
            "Epoch: 58 Step: 6275 Val Loss: 1.6841323375701904\n",
            "Epoch: 58 Step: 6300 Val Loss: 1.6917791366577148\n",
            "Epoch: 58 Step: 6325 Val Loss: 1.7255669832229614\n",
            "Epoch: 58 Step: 6350 Val Loss: 1.7253409624099731\n",
            "Epoch: 59 Step: 6375 Val Loss: 1.6877615451812744\n",
            "Epoch: 59 Step: 6400 Val Loss: 1.668641209602356\n",
            "Epoch: 59 Step: 6425 Val Loss: 1.7082018852233887\n",
            "Epoch: 59 Step: 6450 Val Loss: 1.7006773948669434\n",
            "Epoch: 59 Step: 6475 Val Loss: 1.6896984577178955\n",
            "Epoch: 60 Step: 6500 Val Loss: 1.6690713167190552\n",
            "Epoch: 60 Step: 6525 Val Loss: 1.6965702772140503\n",
            "Epoch: 60 Step: 6550 Val Loss: 1.6868863105773926\n",
            "Epoch: 60 Step: 6575 Val Loss: 1.6991609334945679\n",
            "Epoch: 61 Step: 6600 Val Loss: 1.6575204133987427\n",
            "Epoch: 61 Step: 6625 Val Loss: 1.6646984815597534\n",
            "Epoch: 61 Step: 6650 Val Loss: 1.6883502006530762\n",
            "Epoch: 61 Step: 6675 Val Loss: 1.6950753927230835\n",
            "Epoch: 62 Step: 6700 Val Loss: 1.6588996648788452\n",
            "Epoch: 62 Step: 6725 Val Loss: 1.6594045162200928\n",
            "Epoch: 62 Step: 6750 Val Loss: 1.6808313131332397\n",
            "Epoch: 62 Step: 6775 Val Loss: 1.6723421812057495\n",
            "Epoch: 62 Step: 6800 Val Loss: 1.6689366102218628\n",
            "Epoch: 63 Step: 6825 Val Loss: 1.642102599143982\n",
            "Epoch: 63 Step: 6850 Val Loss: 1.659233570098877\n",
            "Epoch: 63 Step: 6875 Val Loss: 1.655761957168579\n",
            "Epoch: 63 Step: 6900 Val Loss: 1.6560537815093994\n",
            "Epoch: 64 Step: 6925 Val Loss: 1.6286455392837524\n",
            "Epoch: 64 Step: 6950 Val Loss: 1.643886685371399\n",
            "Epoch: 64 Step: 6975 Val Loss: 1.6620795726776123\n",
            "Epoch: 64 Step: 7000 Val Loss: 1.668570876121521\n",
            "Epoch: 65 Step: 7025 Val Loss: 1.6364792585372925\n",
            "Epoch: 65 Step: 7050 Val Loss: 1.6299500465393066\n",
            "Epoch: 65 Step: 7075 Val Loss: 1.657091736793518\n",
            "Epoch: 65 Step: 7100 Val Loss: 1.6438649892807007\n",
            "Epoch: 65 Step: 7125 Val Loss: 1.63787841796875\n",
            "Epoch: 66 Step: 7150 Val Loss: 1.6178594827651978\n",
            "Epoch: 66 Step: 7175 Val Loss: 1.6468437910079956\n",
            "Epoch: 66 Step: 7200 Val Loss: 1.640106201171875\n",
            "Epoch: 66 Step: 7225 Val Loss: 1.6406115293502808\n",
            "Epoch: 67 Step: 7250 Val Loss: 1.6151055097579956\n",
            "Epoch: 67 Step: 7275 Val Loss: 1.6263989210128784\n",
            "Epoch: 67 Step: 7300 Val Loss: 1.627169132232666\n",
            "Epoch: 67 Step: 7325 Val Loss: 1.6334223747253418\n",
            "Epoch: 68 Step: 7350 Val Loss: 1.6137224435806274\n",
            "Epoch: 68 Step: 7375 Val Loss: 1.6107943058013916\n",
            "Epoch: 68 Step: 7400 Val Loss: 1.6329922676086426\n",
            "Epoch: 68 Step: 7425 Val Loss: 1.6272279024124146\n",
            "Epoch: 68 Step: 7450 Val Loss: 1.6294564008712769\n",
            "Epoch: 69 Step: 7475 Val Loss: 1.6042965650558472\n",
            "Epoch: 69 Step: 7500 Val Loss: 1.6317843198776245\n",
            "Epoch: 69 Step: 7525 Val Loss: 1.622744083404541\n",
            "Epoch: 69 Step: 7550 Val Loss: 1.6205580234527588\n",
            "Epoch: 70 Step: 7575 Val Loss: 1.6005644798278809\n",
            "Epoch: 70 Step: 7600 Val Loss: 1.6062781810760498\n",
            "Epoch: 70 Step: 7625 Val Loss: 1.6096677780151367\n",
            "Epoch: 70 Step: 7650 Val Loss: 1.6300582885742188\n",
            "Epoch: 71 Step: 7675 Val Loss: 1.598490595817566\n",
            "Epoch: 71 Step: 7700 Val Loss: 1.5977848768234253\n",
            "Epoch: 71 Step: 7725 Val Loss: 1.6023542881011963\n",
            "Epoch: 71 Step: 7750 Val Loss: 1.6078290939331055\n",
            "Epoch: 71 Step: 7775 Val Loss: 1.6014204025268555\n",
            "Epoch: 72 Step: 7800 Val Loss: 1.5959185361862183\n",
            "Epoch: 72 Step: 7825 Val Loss: 1.618579387664795\n",
            "Epoch: 72 Step: 7850 Val Loss: 1.6010570526123047\n",
            "Epoch: 72 Step: 7875 Val Loss: 1.6008137464523315\n",
            "Epoch: 73 Step: 7900 Val Loss: 1.5844088792800903\n",
            "Epoch: 73 Step: 7925 Val Loss: 1.6052623987197876\n",
            "Epoch: 73 Step: 7950 Val Loss: 1.6013107299804688\n",
            "Epoch: 73 Step: 7975 Val Loss: 1.6077243089675903\n",
            "Epoch: 74 Step: 8000 Val Loss: 1.5799520015716553\n",
            "Epoch: 74 Step: 8025 Val Loss: 1.5827151536941528\n",
            "Epoch: 74 Step: 8050 Val Loss: 1.5837610960006714\n",
            "Epoch: 74 Step: 8075 Val Loss: 1.592947006225586\n",
            "Epoch: 74 Step: 8100 Val Loss: 1.5843844413757324\n",
            "Epoch: 75 Step: 8125 Val Loss: 1.5813114643096924\n",
            "Epoch: 75 Step: 8150 Val Loss: 1.599880337715149\n",
            "Epoch: 75 Step: 8175 Val Loss: 1.592631220817566\n",
            "Epoch: 75 Step: 8200 Val Loss: 1.5907763242721558\n",
            "Epoch: 76 Step: 8225 Val Loss: 1.5709186792373657\n",
            "Epoch: 76 Step: 8250 Val Loss: 1.5768386125564575\n",
            "Epoch: 76 Step: 8275 Val Loss: 1.579391598701477\n",
            "Epoch: 76 Step: 8300 Val Loss: 1.5915889739990234\n",
            "Epoch: 77 Step: 8325 Val Loss: 1.5814887285232544\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 77 Step: 8350 Val Loss: 1.5800155401229858\n",
            "Epoch: 77 Step: 8375 Val Loss: 1.5806851387023926\n",
            "Epoch: 77 Step: 8400 Val Loss: 1.5799442529678345\n",
            "Epoch: 78 Step: 8425 Val Loss: 1.565329909324646\n",
            "Epoch: 78 Step: 8450 Val Loss: 1.5648024082183838\n",
            "Epoch: 78 Step: 8475 Val Loss: 1.5848958492279053\n",
            "Epoch: 78 Step: 8500 Val Loss: 1.5742727518081665\n",
            "Epoch: 78 Step: 8525 Val Loss: 1.5690034627914429\n",
            "Epoch: 79 Step: 8550 Val Loss: 1.557605266571045\n",
            "Epoch: 79 Step: 8575 Val Loss: 1.56142258644104\n",
            "Epoch: 79 Step: 8600 Val Loss: 1.5612282752990723\n",
            "Epoch: 79 Step: 8625 Val Loss: 1.5756860971450806\n",
            "Epoch: 80 Step: 8650 Val Loss: 1.5561553239822388\n",
            "Epoch: 80 Step: 8675 Val Loss: 1.560204267501831\n",
            "Epoch: 80 Step: 8700 Val Loss: 1.5672948360443115\n",
            "Epoch: 80 Step: 8725 Val Loss: 1.5674248933792114\n",
            "Epoch: 81 Step: 8750 Val Loss: 1.5596959590911865\n",
            "Epoch: 81 Step: 8775 Val Loss: 1.5639289617538452\n",
            "Epoch: 81 Step: 8800 Val Loss: 1.5645986795425415\n",
            "Epoch: 81 Step: 8825 Val Loss: 1.5629208087921143\n",
            "Epoch: 81 Step: 8850 Val Loss: 1.5566128492355347\n",
            "Epoch: 82 Step: 8875 Val Loss: 1.541035771369934\n",
            "Epoch: 82 Step: 8900 Val Loss: 1.5561833381652832\n",
            "Epoch: 82 Step: 8925 Val Loss: 1.564940094947815\n",
            "Epoch: 82 Step: 8950 Val Loss: 1.5738314390182495\n",
            "Epoch: 83 Step: 8975 Val Loss: 1.5461852550506592\n",
            "Epoch: 83 Step: 9000 Val Loss: 1.5495034456253052\n",
            "Epoch: 83 Step: 9025 Val Loss: 1.5552667379379272\n",
            "Epoch: 83 Step: 9050 Val Loss: 1.5577447414398193\n",
            "Epoch: 84 Step: 9075 Val Loss: 1.5460504293441772\n",
            "Epoch: 84 Step: 9100 Val Loss: 1.540934443473816\n",
            "Epoch: 84 Step: 9125 Val Loss: 1.54769766330719\n",
            "Epoch: 84 Step: 9150 Val Loss: 1.5491291284561157\n",
            "Epoch: 84 Step: 9175 Val Loss: 1.5516103506088257\n",
            "Epoch: 85 Step: 9200 Val Loss: 1.5397398471832275\n",
            "Epoch: 85 Step: 9225 Val Loss: 1.5479341745376587\n",
            "Epoch: 85 Step: 9250 Val Loss: 1.5440987348556519\n",
            "Epoch: 85 Step: 9275 Val Loss: 1.5473861694335938\n",
            "Epoch: 86 Step: 9300 Val Loss: 1.534429907798767\n",
            "Epoch: 86 Step: 9325 Val Loss: 1.5472887754440308\n",
            "Epoch: 86 Step: 9350 Val Loss: 1.5435400009155273\n",
            "Epoch: 86 Step: 9375 Val Loss: 1.5548168420791626\n",
            "Epoch: 87 Step: 9400 Val Loss: 1.5368459224700928\n",
            "Epoch: 87 Step: 9425 Val Loss: 1.5380569696426392\n",
            "Epoch: 87 Step: 9450 Val Loss: 1.5499300956726074\n",
            "Epoch: 87 Step: 9475 Val Loss: 1.5377895832061768\n",
            "Epoch: 87 Step: 9500 Val Loss: 1.5361191034317017\n",
            "Epoch: 88 Step: 9525 Val Loss: 1.5276767015457153\n",
            "Epoch: 88 Step: 9550 Val Loss: 1.5392043590545654\n",
            "Epoch: 88 Step: 9575 Val Loss: 1.5335930585861206\n",
            "Epoch: 88 Step: 9600 Val Loss: 1.5432974100112915\n",
            "Epoch: 89 Step: 9625 Val Loss: 1.5248239040374756\n",
            "Epoch: 89 Step: 9650 Val Loss: 1.5333324670791626\n",
            "Epoch: 89 Step: 9675 Val Loss: 1.5341460704803467\n",
            "Epoch: 89 Step: 9700 Val Loss: 1.5447323322296143\n",
            "Epoch: 90 Step: 9725 Val Loss: 1.5298104286193848\n",
            "Epoch: 90 Step: 9750 Val Loss: 1.5288742780685425\n",
            "Epoch: 90 Step: 9775 Val Loss: 1.5363479852676392\n",
            "Epoch: 90 Step: 9800 Val Loss: 1.5371301174163818\n",
            "Epoch: 90 Step: 9825 Val Loss: 1.5272799730300903\n",
            "Epoch: 91 Step: 9850 Val Loss: 1.523642897605896\n",
            "Epoch: 91 Step: 9875 Val Loss: 1.5396777391433716\n",
            "Epoch: 91 Step: 9900 Val Loss: 1.5360794067382812\n",
            "Epoch: 91 Step: 9925 Val Loss: 1.5385494232177734\n",
            "Epoch: 92 Step: 9950 Val Loss: 1.5261856317520142\n",
            "Epoch: 92 Step: 9975 Val Loss: 1.5302425622940063\n",
            "Epoch: 92 Step: 10000 Val Loss: 1.5362014770507812\n",
            "Epoch: 92 Step: 10025 Val Loss: 1.534397840499878\n",
            "Epoch: 93 Step: 10050 Val Loss: 1.5188884735107422\n",
            "Epoch: 93 Step: 10075 Val Loss: 1.5211089849472046\n",
            "Epoch: 93 Step: 10100 Val Loss: 1.5252304077148438\n",
            "Epoch: 93 Step: 10125 Val Loss: 1.5306365489959717\n",
            "Epoch: 93 Step: 10150 Val Loss: 1.5234216451644897\n",
            "Epoch: 94 Step: 10175 Val Loss: 1.5189815759658813\n",
            "Epoch: 94 Step: 10200 Val Loss: 1.5323879718780518\n",
            "Epoch: 94 Step: 10225 Val Loss: 1.523680329322815\n",
            "Epoch: 94 Step: 10250 Val Loss: 1.5270158052444458\n",
            "Epoch: 95 Step: 10275 Val Loss: 1.515910267829895\n",
            "Epoch: 95 Step: 10300 Val Loss: 1.526895523071289\n",
            "Epoch: 95 Step: 10325 Val Loss: 1.5244766473770142\n",
            "Epoch: 95 Step: 10350 Val Loss: 1.5326510667800903\n",
            "Epoch: 96 Step: 10375 Val Loss: 1.5176573991775513\n",
            "Epoch: 96 Step: 10400 Val Loss: 1.520247459411621\n",
            "Epoch: 96 Step: 10425 Val Loss: 1.5266807079315186\n",
            "Epoch: 96 Step: 10450 Val Loss: 1.526749849319458\n",
            "Epoch: 96 Step: 10475 Val Loss: 1.5202471017837524\n",
            "Epoch: 97 Step: 10500 Val Loss: 1.5189965963363647\n",
            "Epoch: 97 Step: 10525 Val Loss: 1.5292307138442993\n",
            "Epoch: 97 Step: 10550 Val Loss: 1.5262137651443481\n",
            "Epoch: 97 Step: 10575 Val Loss: 1.5230388641357422\n",
            "Epoch: 98 Step: 10600 Val Loss: 1.5130186080932617\n",
            "Epoch: 98 Step: 10625 Val Loss: 1.5238131284713745\n",
            "Epoch: 98 Step: 10650 Val Loss: 1.5147672891616821\n",
            "Epoch: 98 Step: 10675 Val Loss: 1.5387786626815796\n",
            "Epoch: 99 Step: 10700 Val Loss: 1.5087330341339111\n",
            "Epoch: 99 Step: 10725 Val Loss: 1.5207144021987915\n",
            "Epoch: 99 Step: 10750 Val Loss: 1.5156790018081665\n",
            "Epoch: 99 Step: 10775 Val Loss: 1.52074134349823\n",
            "Epoch: 99 Step: 10800 Val Loss: 1.5122238397598267\n",
            "Epoch: 100 Step: 10825 Val Loss: 1.5087875127792358\n",
            "Epoch: 100 Step: 10850 Val Loss: 1.529760718345642\n",
            "Epoch: 100 Step: 10875 Val Loss: 1.5202715396881104\n",
            "Epoch: 100 Step: 10900 Val Loss: 1.513167381286621\n",
            "Epoch: 101 Step: 10925 Val Loss: 1.5044784545898438\n",
            "Epoch: 101 Step: 10950 Val Loss: 1.5144377946853638\n",
            "Epoch: 101 Step: 10975 Val Loss: 1.5096209049224854\n",
            "Epoch: 101 Step: 11000 Val Loss: 1.5194802284240723\n",
            "Epoch: 102 Step: 11025 Val Loss: 1.5041134357452393\n",
            "Epoch: 102 Step: 11050 Val Loss: 1.5090769529342651\n",
            "Epoch: 102 Step: 11075 Val Loss: 1.5113893747329712\n",
            "Epoch: 102 Step: 11100 Val Loss: 1.5135904550552368\n",
            "Epoch: 103 Step: 11125 Val Loss: 1.5041569471359253\n",
            "Epoch: 103 Step: 11150 Val Loss: 1.5085865259170532\n",
            "Epoch: 103 Step: 11175 Val Loss: 1.5133870840072632\n",
            "Epoch: 103 Step: 11200 Val Loss: 1.5116509199142456\n",
            "Epoch: 103 Step: 11225 Val Loss: 1.5103832483291626\n",
            "Epoch: 104 Step: 11250 Val Loss: 1.5013396739959717\n",
            "Epoch: 104 Step: 11275 Val Loss: 1.5147857666015625\n",
            "Epoch: 104 Step: 11300 Val Loss: 1.5060590505599976\n",
            "Epoch: 104 Step: 11325 Val Loss: 1.5100359916687012\n",
            "Epoch: 105 Step: 11350 Val Loss: 1.5015032291412354\n",
            "Epoch: 105 Step: 11375 Val Loss: 1.5062686204910278\n",
            "Epoch: 105 Step: 11400 Val Loss: 1.5100141763687134\n",
            "Epoch: 105 Step: 11425 Val Loss: 1.5144643783569336\n",
            "Epoch: 106 Step: 11450 Val Loss: 1.504846215248108\n",
            "Epoch: 106 Step: 11475 Val Loss: 1.5048375129699707\n",
            "Epoch: 106 Step: 11500 Val Loss: 1.5137569904327393\n",
            "Epoch: 106 Step: 11525 Val Loss: 1.5082873106002808\n",
            "Epoch: 106 Step: 11550 Val Loss: 1.5053446292877197\n",
            "Epoch: 107 Step: 11575 Val Loss: 1.5005908012390137\n",
            "Epoch: 107 Step: 11600 Val Loss: 1.5162031650543213\n",
            "Epoch: 107 Step: 11625 Val Loss: 1.5119603872299194\n",
            "Epoch: 107 Step: 11650 Val Loss: 1.5109513998031616\n",
            "Epoch: 108 Step: 11675 Val Loss: 1.5011234283447266\n",
            "Epoch: 108 Step: 11700 Val Loss: 1.5039615631103516\n",
            "Epoch: 108 Step: 11725 Val Loss: 1.4996683597564697\n",
            "Epoch: 108 Step: 11750 Val Loss: 1.5120666027069092\n",
            "Epoch: 109 Step: 11775 Val Loss: 1.4959198236465454\n",
            "Epoch: 109 Step: 11800 Val Loss: 1.4973310232162476\n",
            "Epoch: 109 Step: 11825 Val Loss: 1.5089995861053467\n",
            "Epoch: 109 Step: 11850 Val Loss: 1.5065423250198364\n",
            "Epoch: 109 Step: 11875 Val Loss: 1.5001858472824097\n",
            "Epoch: 110 Step: 11900 Val Loss: 1.4946280717849731\n",
            "Epoch: 110 Step: 11925 Val Loss: 1.5142731666564941\n",
            "Epoch: 110 Step: 11950 Val Loss: 1.4996439218521118\n",
            "Epoch: 110 Step: 11975 Val Loss: 1.5056748390197754\n",
            "Epoch: 111 Step: 12000 Val Loss: 1.4945727586746216\n",
            "Epoch: 111 Step: 12025 Val Loss: 1.504516839981079\n",
            "Epoch: 111 Step: 12050 Val Loss: 1.50740385055542\n",
            "Epoch: 111 Step: 12075 Val Loss: 1.505539059638977\n",
            "Epoch: 112 Step: 12100 Val Loss: 1.4929330348968506\n",
            "Epoch: 112 Step: 12125 Val Loss: 1.497143268585205\n",
            "Epoch: 112 Step: 12150 Val Loss: 1.505224585533142\n",
            "Epoch: 112 Step: 12175 Val Loss: 1.5043951272964478\n",
            "Epoch: 112 Step: 12200 Val Loss: 1.4958521127700806\n",
            "Epoch: 113 Step: 12225 Val Loss: 1.491927146911621\n",
            "Epoch: 113 Step: 12250 Val Loss: 1.5061880350112915\n",
            "Epoch: 113 Step: 12275 Val Loss: 1.4984031915664673\n",
            "Epoch: 113 Step: 12300 Val Loss: 1.5006762742996216\n",
            "Epoch: 114 Step: 12325 Val Loss: 1.4904519319534302\n",
            "Epoch: 114 Step: 12350 Val Loss: 1.5010453462600708\n",
            "Epoch: 114 Step: 12375 Val Loss: 1.5008982419967651\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 114 Step: 12400 Val Loss: 1.5078980922698975\n",
            "Epoch: 115 Step: 12425 Val Loss: 1.4928823709487915\n",
            "Epoch: 115 Step: 12450 Val Loss: 1.4976779222488403\n",
            "Epoch: 115 Step: 12475 Val Loss: 1.5026192665100098\n",
            "Epoch: 115 Step: 12500 Val Loss: 1.5040398836135864\n",
            "Epoch: 115 Step: 12525 Val Loss: 1.4932411909103394\n",
            "Epoch: 116 Step: 12550 Val Loss: 1.490884780883789\n",
            "Epoch: 116 Step: 12575 Val Loss: 1.5036674737930298\n",
            "Epoch: 116 Step: 12600 Val Loss: 1.49900484085083\n",
            "Epoch: 116 Step: 12625 Val Loss: 1.5009026527404785\n",
            "Epoch: 117 Step: 12650 Val Loss: 1.4842610359191895\n",
            "Epoch: 117 Step: 12675 Val Loss: 1.498829960823059\n",
            "Epoch: 117 Step: 12700 Val Loss: 1.4911468029022217\n",
            "Epoch: 117 Step: 12725 Val Loss: 1.5017211437225342\n",
            "Epoch: 118 Step: 12750 Val Loss: 1.4955227375030518\n",
            "Epoch: 118 Step: 12775 Val Loss: 1.49647855758667\n",
            "Epoch: 118 Step: 12800 Val Loss: 1.501038670539856\n",
            "Epoch: 118 Step: 12825 Val Loss: 1.4992364645004272\n",
            "Epoch: 118 Step: 12850 Val Loss: 1.4904817342758179\n",
            "Epoch: 119 Step: 12875 Val Loss: 1.4895275831222534\n",
            "Epoch: 119 Step: 12900 Val Loss: 1.4972949028015137\n",
            "Epoch: 119 Step: 12925 Val Loss: 1.4936994314193726\n",
            "Epoch: 119 Step: 12950 Val Loss: 1.4934130907058716\n",
            "Epoch: 120 Step: 12975 Val Loss: 1.4851166009902954\n",
            "Epoch: 120 Step: 13000 Val Loss: 1.4975380897521973\n",
            "Epoch: 120 Step: 13025 Val Loss: 1.4964940547943115\n",
            "Epoch: 120 Step: 13050 Val Loss: 1.5061334371566772\n",
            "Epoch: 121 Step: 13075 Val Loss: 1.4867833852767944\n",
            "Epoch: 121 Step: 13100 Val Loss: 1.4920589923858643\n",
            "Epoch: 121 Step: 13125 Val Loss: 1.4933173656463623\n",
            "Epoch: 121 Step: 13150 Val Loss: 1.4961373805999756\n",
            "Epoch: 121 Step: 13175 Val Loss: 1.48939049243927\n",
            "Epoch: 122 Step: 13200 Val Loss: 1.4905427694320679\n",
            "Epoch: 122 Step: 13225 Val Loss: 1.4975963830947876\n",
            "Epoch: 122 Step: 13250 Val Loss: 1.4986310005187988\n",
            "Epoch: 122 Step: 13275 Val Loss: 1.49551260471344\n",
            "Epoch: 123 Step: 13300 Val Loss: 1.4792965650558472\n",
            "Epoch: 123 Step: 13325 Val Loss: 1.4889721870422363\n",
            "Epoch: 123 Step: 13350 Val Loss: 1.4916714429855347\n",
            "Epoch: 123 Step: 13375 Val Loss: 1.497618317604065\n",
            "Epoch: 124 Step: 13400 Val Loss: 1.4851888418197632\n",
            "Epoch: 124 Step: 13425 Val Loss: 1.4884346723556519\n",
            "Epoch: 124 Step: 13450 Val Loss: 1.486876368522644\n",
            "Epoch: 124 Step: 13475 Val Loss: 1.4922066926956177\n",
            "Epoch: 124 Step: 13500 Val Loss: 1.4841598272323608\n",
            "Epoch: 125 Step: 13525 Val Loss: 1.4991451501846313\n",
            "Epoch: 125 Step: 13550 Val Loss: 1.5010555982589722\n",
            "Epoch: 125 Step: 13575 Val Loss: 1.4881713390350342\n",
            "Epoch: 125 Step: 13600 Val Loss: 1.4890079498291016\n",
            "Epoch: 126 Step: 13625 Val Loss: 1.4785860776901245\n",
            "Epoch: 126 Step: 13650 Val Loss: 1.4845293760299683\n",
            "Epoch: 126 Step: 13675 Val Loss: 1.481529951095581\n",
            "Epoch: 126 Step: 13700 Val Loss: 1.4923356771469116\n",
            "Epoch: 127 Step: 13725 Val Loss: 1.4783254861831665\n",
            "Epoch: 127 Step: 13750 Val Loss: 1.488100290298462\n",
            "Epoch: 127 Step: 13775 Val Loss: 1.4894012212753296\n",
            "Epoch: 127 Step: 13800 Val Loss: 1.489146113395691\n",
            "Epoch: 128 Step: 13825 Val Loss: 1.4794775247573853\n",
            "Epoch: 128 Step: 13850 Val Loss: 1.4842242002487183\n",
            "Epoch: 128 Step: 13875 Val Loss: 1.4884217977523804\n",
            "Epoch: 128 Step: 13900 Val Loss: 1.4947232007980347\n",
            "Epoch: 128 Step: 13925 Val Loss: 1.4823799133300781\n",
            "Epoch: 129 Step: 13950 Val Loss: 1.4771310091018677\n",
            "Epoch: 129 Step: 13975 Val Loss: 1.4861599206924438\n",
            "Epoch: 129 Step: 14000 Val Loss: 1.4822477102279663\n",
            "Epoch: 129 Step: 14025 Val Loss: 1.4894373416900635\n",
            "Epoch: 130 Step: 14050 Val Loss: 1.4755922555923462\n",
            "Epoch: 130 Step: 14075 Val Loss: 1.4921727180480957\n",
            "Epoch: 130 Step: 14100 Val Loss: 1.4989794492721558\n",
            "Epoch: 130 Step: 14125 Val Loss: 1.4948005676269531\n",
            "Epoch: 131 Step: 14150 Val Loss: 1.4840993881225586\n",
            "Epoch: 131 Step: 14175 Val Loss: 1.4852224588394165\n",
            "Epoch: 131 Step: 14200 Val Loss: 1.486035704612732\n",
            "Epoch: 131 Step: 14225 Val Loss: 1.4867193698883057\n",
            "Epoch: 131 Step: 14250 Val Loss: 1.4810640811920166\n",
            "Epoch: 132 Step: 14275 Val Loss: 1.4737298488616943\n",
            "Epoch: 132 Step: 14300 Val Loss: 1.483864665031433\n",
            "Epoch: 132 Step: 14325 Val Loss: 1.4800493717193604\n",
            "Epoch: 132 Step: 14350 Val Loss: 1.4821487665176392\n",
            "Epoch: 133 Step: 14375 Val Loss: 1.4702825546264648\n",
            "Epoch: 133 Step: 14400 Val Loss: 1.4779101610183716\n",
            "Epoch: 133 Step: 14425 Val Loss: 1.481451392173767\n",
            "Epoch: 133 Step: 14450 Val Loss: 1.4823054075241089\n",
            "Epoch: 134 Step: 14475 Val Loss: 1.4763257503509521\n",
            "Epoch: 134 Step: 14500 Val Loss: 1.485691785812378\n",
            "Epoch: 134 Step: 14525 Val Loss: 1.48557448387146\n",
            "Epoch: 134 Step: 14550 Val Loss: 1.4937430620193481\n",
            "Epoch: 134 Step: 14575 Val Loss: 1.483519434928894\n",
            "Epoch: 135 Step: 14600 Val Loss: 1.4735747575759888\n",
            "Epoch: 135 Step: 14625 Val Loss: 1.484987497329712\n",
            "Epoch: 135 Step: 14650 Val Loss: 1.4775704145431519\n",
            "Epoch: 135 Step: 14675 Val Loss: 1.4834365844726562\n",
            "Epoch: 136 Step: 14700 Val Loss: 1.4751931428909302\n",
            "Epoch: 136 Step: 14725 Val Loss: 1.4818568229675293\n",
            "Epoch: 136 Step: 14750 Val Loss: 1.4809212684631348\n",
            "Epoch: 136 Step: 14775 Val Loss: 1.4847713708877563\n",
            "Epoch: 137 Step: 14800 Val Loss: 1.4745104312896729\n",
            "Epoch: 137 Step: 14825 Val Loss: 1.4794427156448364\n",
            "Epoch: 137 Step: 14850 Val Loss: 1.4812893867492676\n",
            "Epoch: 137 Step: 14875 Val Loss: 1.481126070022583\n",
            "Epoch: 137 Step: 14900 Val Loss: 1.4749104976654053\n",
            "Epoch: 138 Step: 14925 Val Loss: 1.4729937314987183\n",
            "Epoch: 138 Step: 14950 Val Loss: 1.4841293096542358\n",
            "Epoch: 138 Step: 14975 Val Loss: 1.483697533607483\n",
            "Epoch: 138 Step: 15000 Val Loss: 1.479921817779541\n",
            "Epoch: 139 Step: 15025 Val Loss: 1.471190094947815\n",
            "Epoch: 139 Step: 15050 Val Loss: 1.4843522310256958\n",
            "Epoch: 139 Step: 15075 Val Loss: 1.4808727502822876\n",
            "Epoch: 139 Step: 15100 Val Loss: 1.4904640913009644\n",
            "Epoch: 140 Step: 15125 Val Loss: 1.4758821725845337\n",
            "Epoch: 140 Step: 15150 Val Loss: 1.4783670902252197\n",
            "Epoch: 140 Step: 15175 Val Loss: 1.481778860092163\n",
            "Epoch: 140 Step: 15200 Val Loss: 1.479767084121704\n",
            "Epoch: 140 Step: 15225 Val Loss: 1.4719816446304321\n",
            "Epoch: 141 Step: 15250 Val Loss: 1.4755035638809204\n",
            "Epoch: 141 Step: 15275 Val Loss: 1.4804528951644897\n",
            "Epoch: 141 Step: 15300 Val Loss: 1.4797382354736328\n",
            "Epoch: 141 Step: 15325 Val Loss: 1.4773790836334229\n",
            "Epoch: 142 Step: 15350 Val Loss: 1.4649828672409058\n",
            "Epoch: 142 Step: 15375 Val Loss: 1.4764434099197388\n",
            "Epoch: 142 Step: 15400 Val Loss: 1.4718443155288696\n",
            "Epoch: 142 Step: 15425 Val Loss: 1.4827523231506348\n",
            "Epoch: 143 Step: 15450 Val Loss: 1.470064640045166\n",
            "Epoch: 143 Step: 15475 Val Loss: 1.5047200918197632\n",
            "Epoch: 143 Step: 15500 Val Loss: 1.4766868352890015\n",
            "Epoch: 143 Step: 15525 Val Loss: 1.475702166557312\n",
            "Epoch: 143 Step: 15550 Val Loss: 1.4711943864822388\n",
            "Epoch: 144 Step: 15575 Val Loss: 1.4729186296463013\n",
            "Epoch: 144 Step: 15600 Val Loss: 1.4808040857315063\n",
            "Epoch: 144 Step: 15625 Val Loss: 1.4795864820480347\n",
            "Epoch: 144 Step: 15650 Val Loss: 1.4769017696380615\n",
            "Epoch: 145 Step: 15675 Val Loss: 1.4697250127792358\n",
            "Epoch: 145 Step: 15700 Val Loss: 1.4738012552261353\n",
            "Epoch: 145 Step: 15725 Val Loss: 1.4758672714233398\n",
            "Epoch: 145 Step: 15750 Val Loss: 1.4811632633209229\n",
            "Epoch: 146 Step: 15775 Val Loss: 1.4702646732330322\n",
            "Epoch: 146 Step: 15800 Val Loss: 1.4738608598709106\n",
            "Epoch: 146 Step: 15825 Val Loss: 1.4769419431686401\n",
            "Epoch: 146 Step: 15850 Val Loss: 1.4791662693023682\n",
            "Epoch: 146 Step: 15875 Val Loss: 1.4715502262115479\n",
            "Epoch: 147 Step: 15900 Val Loss: 1.4702489376068115\n",
            "Epoch: 147 Step: 15925 Val Loss: 1.4761096239089966\n",
            "Epoch: 147 Step: 15950 Val Loss: 1.4755407571792603\n",
            "Epoch: 147 Step: 15975 Val Loss: 1.4703317880630493\n",
            "Epoch: 148 Step: 16000 Val Loss: 1.4665666818618774\n",
            "Epoch: 148 Step: 16025 Val Loss: 1.475587010383606\n",
            "Epoch: 148 Step: 16050 Val Loss: 1.4702775478363037\n",
            "Epoch: 148 Step: 16075 Val Loss: 1.4752697944641113\n",
            "Epoch: 149 Step: 16100 Val Loss: 1.464714527130127\n",
            "Epoch: 149 Step: 16125 Val Loss: 1.4658548831939697\n",
            "Epoch: 149 Step: 16150 Val Loss: 1.4685180187225342\n",
            "Epoch: 149 Step: 16175 Val Loss: 1.4696645736694336\n",
            "Epoch: 149 Step: 16200 Val Loss: 1.4642661809921265\n",
            "Epoch: 150 Step: 16225 Val Loss: 1.4644027948379517\n",
            "Epoch: 150 Step: 16250 Val Loss: 1.4695686101913452\n",
            "Epoch: 150 Step: 16275 Val Loss: 1.4716335535049438\n",
            "Epoch: 150 Step: 16300 Val Loss: 1.4654182195663452\n",
            "Epoch: 151 Step: 16325 Val Loss: 1.465185523033142\n",
            "Epoch: 151 Step: 16350 Val Loss: 1.4783450365066528\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 151 Step: 16375 Val Loss: 1.4815938472747803\n",
            "Epoch: 151 Step: 16400 Val Loss: 1.4819433689117432\n",
            "Epoch: 152 Step: 16425 Val Loss: 1.466693639755249\n",
            "Epoch: 152 Step: 16450 Val Loss: 1.4728907346725464\n",
            "Epoch: 152 Step: 16475 Val Loss: 1.473405122756958\n",
            "Epoch: 152 Step: 16500 Val Loss: 1.4743791818618774\n",
            "Epoch: 153 Step: 16525 Val Loss: 1.4681147336959839\n",
            "Epoch: 153 Step: 16550 Val Loss: 1.474959373474121\n",
            "Epoch: 153 Step: 16575 Val Loss: 1.4814121723175049\n",
            "Epoch: 153 Step: 16600 Val Loss: 1.4740285873413086\n",
            "Epoch: 153 Step: 16625 Val Loss: 1.4677218198776245\n",
            "Epoch: 154 Step: 16650 Val Loss: 1.4612423181533813\n",
            "Epoch: 154 Step: 16675 Val Loss: 1.4711287021636963\n",
            "Epoch: 154 Step: 16700 Val Loss: 1.4680534601211548\n",
            "Epoch: 154 Step: 16725 Val Loss: 1.471130609512329\n",
            "Epoch: 155 Step: 16750 Val Loss: 1.4615777730941772\n",
            "Epoch: 155 Step: 16775 Val Loss: 1.4632046222686768\n",
            "Epoch: 155 Step: 16800 Val Loss: 1.4647600650787354\n",
            "Epoch: 155 Step: 16825 Val Loss: 1.4744967222213745\n",
            "Epoch: 156 Step: 16850 Val Loss: 1.4641910791397095\n",
            "Epoch: 156 Step: 16875 Val Loss: 1.4613195657730103\n",
            "Epoch: 156 Step: 16900 Val Loss: 1.4645135402679443\n",
            "Epoch: 156 Step: 16925 Val Loss: 1.4750083684921265\n",
            "Epoch: 156 Step: 16950 Val Loss: 1.462884783744812\n",
            "Epoch: 157 Step: 16975 Val Loss: 1.4587740898132324\n",
            "Epoch: 157 Step: 17000 Val Loss: 1.4684102535247803\n",
            "Epoch: 157 Step: 17025 Val Loss: 1.4633578062057495\n",
            "Epoch: 157 Step: 17050 Val Loss: 1.4710009098052979\n",
            "Epoch: 158 Step: 17075 Val Loss: 1.4602453708648682\n",
            "Epoch: 158 Step: 17100 Val Loss: 1.4674962759017944\n",
            "Epoch: 158 Step: 17125 Val Loss: 1.4682236909866333\n",
            "Epoch: 158 Step: 17150 Val Loss: 1.474037766456604\n",
            "Epoch: 159 Step: 17175 Val Loss: 1.462358832359314\n",
            "Epoch: 159 Step: 17200 Val Loss: 1.467265009880066\n",
            "Epoch: 159 Step: 17225 Val Loss: 1.4739853143692017\n",
            "Epoch: 159 Step: 17250 Val Loss: 1.4676711559295654\n",
            "Epoch: 159 Step: 17275 Val Loss: 1.4673746824264526\n",
            "Epoch: 160 Step: 17300 Val Loss: 1.4631315469741821\n",
            "Epoch: 160 Step: 17325 Val Loss: 1.4640647172927856\n",
            "Epoch: 160 Step: 17350 Val Loss: 1.463428258895874\n",
            "Epoch: 160 Step: 17375 Val Loss: 1.463222622871399\n",
            "Epoch: 161 Step: 17400 Val Loss: 1.4580004215240479\n",
            "Epoch: 161 Step: 17425 Val Loss: 1.46285879611969\n",
            "Epoch: 161 Step: 17450 Val Loss: 1.4608232975006104\n",
            "Epoch: 161 Step: 17475 Val Loss: 1.4679137468338013\n",
            "Epoch: 162 Step: 17500 Val Loss: 1.461049199104309\n",
            "Epoch: 162 Step: 17525 Val Loss: 1.4627467393875122\n",
            "Epoch: 162 Step: 17550 Val Loss: 1.4703757762908936\n",
            "Epoch: 162 Step: 17575 Val Loss: 1.4653762578964233\n",
            "Epoch: 162 Step: 17600 Val Loss: 1.4605523347854614\n",
            "Epoch: 163 Step: 17625 Val Loss: 1.458467721939087\n",
            "Epoch: 163 Step: 17650 Val Loss: 1.4676209688186646\n",
            "Epoch: 163 Step: 17675 Val Loss: 1.4639180898666382\n",
            "Epoch: 163 Step: 17700 Val Loss: 1.4635010957717896\n",
            "Epoch: 164 Step: 17725 Val Loss: 1.4588838815689087\n",
            "Epoch: 164 Step: 17750 Val Loss: 1.4685208797454834\n",
            "Epoch: 164 Step: 17775 Val Loss: 1.4607958793640137\n",
            "Epoch: 164 Step: 17800 Val Loss: 1.4748077392578125\n",
            "Epoch: 165 Step: 17825 Val Loss: 1.4630111455917358\n",
            "Epoch: 165 Step: 17850 Val Loss: 1.4630845785140991\n",
            "Epoch: 165 Step: 17875 Val Loss: 1.4739959239959717\n",
            "Epoch: 165 Step: 17900 Val Loss: 1.4716242551803589\n",
            "Epoch: 165 Step: 17925 Val Loss: 1.4618843793869019\n",
            "Epoch: 166 Step: 17950 Val Loss: 1.4584121704101562\n",
            "Epoch: 166 Step: 17975 Val Loss: 1.4590208530426025\n",
            "Epoch: 166 Step: 18000 Val Loss: 1.4568315744400024\n",
            "Epoch: 166 Step: 18025 Val Loss: 1.4590587615966797\n",
            "Epoch: 167 Step: 18050 Val Loss: 1.45431387424469\n",
            "Epoch: 167 Step: 18075 Val Loss: 1.4644311666488647\n",
            "Epoch: 167 Step: 18100 Val Loss: 1.470462441444397\n",
            "Epoch: 167 Step: 18125 Val Loss: 1.4745784997940063\n",
            "Epoch: 168 Step: 18150 Val Loss: 1.457056999206543\n",
            "Epoch: 168 Step: 18175 Val Loss: 1.4619121551513672\n",
            "Epoch: 168 Step: 18200 Val Loss: 1.4634205102920532\n",
            "Epoch: 168 Step: 18225 Val Loss: 1.4595799446105957\n",
            "Epoch: 168 Step: 18250 Val Loss: 1.4609431028366089\n",
            "Epoch: 169 Step: 18275 Val Loss: 1.5011364221572876\n",
            "Epoch: 169 Step: 18300 Val Loss: 1.5039021968841553\n",
            "Epoch: 169 Step: 18325 Val Loss: 1.4689602851867676\n",
            "Epoch: 169 Step: 18350 Val Loss: 1.465113639831543\n",
            "Epoch: 170 Step: 18375 Val Loss: 1.4589039087295532\n",
            "Epoch: 170 Step: 18400 Val Loss: 1.4673981666564941\n",
            "Epoch: 170 Step: 18425 Val Loss: 1.466912865638733\n",
            "Epoch: 170 Step: 18450 Val Loss: 1.470559000968933\n",
            "Epoch: 171 Step: 18475 Val Loss: 1.461626410484314\n",
            "Epoch: 171 Step: 18500 Val Loss: 1.463220238685608\n",
            "Epoch: 171 Step: 18525 Val Loss: 1.4650077819824219\n",
            "Epoch: 171 Step: 18550 Val Loss: 1.462138056755066\n",
            "Epoch: 171 Step: 18575 Val Loss: 1.458609938621521\n",
            "Epoch: 172 Step: 18600 Val Loss: 1.4572341442108154\n",
            "Epoch: 172 Step: 18625 Val Loss: 1.4651857614517212\n",
            "Epoch: 172 Step: 18650 Val Loss: 1.4675147533416748\n",
            "Epoch: 172 Step: 18675 Val Loss: 1.4586749076843262\n",
            "Epoch: 173 Step: 18700 Val Loss: 1.4552303552627563\n",
            "Epoch: 173 Step: 18725 Val Loss: 1.4582809209823608\n",
            "Epoch: 173 Step: 18750 Val Loss: 1.4553762674331665\n",
            "Epoch: 173 Step: 18775 Val Loss: 1.4621645212173462\n",
            "Epoch: 174 Step: 18800 Val Loss: 1.4539055824279785\n",
            "Epoch: 174 Step: 18825 Val Loss: 1.4584636688232422\n",
            "Epoch: 174 Step: 18850 Val Loss: 1.4620062112808228\n",
            "Epoch: 174 Step: 18875 Val Loss: 1.4630985260009766\n",
            "Epoch: 174 Step: 18900 Val Loss: 1.4537851810455322\n",
            "Epoch: 175 Step: 18925 Val Loss: 1.458087682723999\n",
            "Epoch: 175 Step: 18950 Val Loss: 1.4656972885131836\n",
            "Epoch: 175 Step: 18975 Val Loss: 1.4580179452896118\n",
            "Epoch: 175 Step: 19000 Val Loss: 1.4553879499435425\n",
            "Epoch: 176 Step: 19025 Val Loss: 1.4511693716049194\n",
            "Epoch: 176 Step: 19050 Val Loss: 1.4600450992584229\n",
            "Epoch: 176 Step: 19075 Val Loss: 1.4582067728042603\n",
            "Epoch: 176 Step: 19100 Val Loss: 1.464781403541565\n",
            "Epoch: 177 Step: 19125 Val Loss: 1.4522500038146973\n",
            "Epoch: 177 Step: 19150 Val Loss: 1.4574079513549805\n",
            "Epoch: 177 Step: 19175 Val Loss: 1.4652323722839355\n",
            "Epoch: 177 Step: 19200 Val Loss: 1.4548437595367432\n",
            "Epoch: 178 Step: 19225 Val Loss: 1.4500445127487183\n",
            "Epoch: 178 Step: 19250 Val Loss: 1.4524405002593994\n",
            "Epoch: 178 Step: 19275 Val Loss: 1.4677178859710693\n",
            "Epoch: 178 Step: 19300 Val Loss: 1.4631421566009521\n",
            "Epoch: 178 Step: 19325 Val Loss: 1.4549881219863892\n",
            "Epoch: 179 Step: 19350 Val Loss: 1.4520353078842163\n",
            "Epoch: 179 Step: 19375 Val Loss: 1.4585176706314087\n",
            "Epoch: 179 Step: 19400 Val Loss: 1.457121729850769\n",
            "Epoch: 179 Step: 19425 Val Loss: 1.4607257843017578\n",
            "Epoch: 180 Step: 19450 Val Loss: 1.4508215188980103\n",
            "Epoch: 180 Step: 19475 Val Loss: 1.458339810371399\n",
            "Epoch: 180 Step: 19500 Val Loss: 1.4580379724502563\n",
            "Epoch: 180 Step: 19525 Val Loss: 1.4570080041885376\n",
            "Epoch: 181 Step: 19550 Val Loss: 1.4506032466888428\n",
            "Epoch: 181 Step: 19575 Val Loss: 1.4545081853866577\n",
            "Epoch: 181 Step: 19600 Val Loss: 1.469477653503418\n",
            "Epoch: 181 Step: 19625 Val Loss: 1.465319037437439\n",
            "Epoch: 181 Step: 19650 Val Loss: 1.4538629055023193\n",
            "Epoch: 182 Step: 19675 Val Loss: 1.4500032663345337\n",
            "Epoch: 182 Step: 19700 Val Loss: 1.4597183465957642\n",
            "Epoch: 182 Step: 19725 Val Loss: 1.456472396850586\n",
            "Epoch: 182 Step: 19750 Val Loss: 1.4604008197784424\n",
            "Epoch: 183 Step: 19775 Val Loss: 1.4511092901229858\n",
            "Epoch: 183 Step: 19800 Val Loss: 1.4529184103012085\n",
            "Epoch: 183 Step: 19825 Val Loss: 1.4542826414108276\n",
            "Epoch: 183 Step: 19850 Val Loss: 1.455039381980896\n",
            "Epoch: 184 Step: 19875 Val Loss: 1.443839430809021\n",
            "Epoch: 184 Step: 19900 Val Loss: 1.4501469135284424\n",
            "Epoch: 184 Step: 19925 Val Loss: 1.460100531578064\n",
            "Epoch: 184 Step: 19950 Val Loss: 1.4625965356826782\n",
            "Epoch: 184 Step: 19975 Val Loss: 1.4505479335784912\n",
            "Epoch: 185 Step: 20000 Val Loss: 1.44414484500885\n",
            "Epoch: 185 Step: 20025 Val Loss: 1.4467339515686035\n",
            "Epoch: 185 Step: 20050 Val Loss: 1.448609471321106\n",
            "Epoch: 185 Step: 20075 Val Loss: 1.4512765407562256\n",
            "Epoch: 186 Step: 20100 Val Loss: 1.4493916034698486\n",
            "Epoch: 186 Step: 20125 Val Loss: 1.4524765014648438\n",
            "Epoch: 186 Step: 20150 Val Loss: 1.4514083862304688\n",
            "Epoch: 186 Step: 20175 Val Loss: 1.4538713693618774\n",
            "Epoch: 187 Step: 20200 Val Loss: 1.4472204446792603\n",
            "Epoch: 187 Step: 20225 Val Loss: 1.4500490427017212\n",
            "Epoch: 187 Step: 20250 Val Loss: 1.4509786367416382\n",
            "Epoch: 187 Step: 20275 Val Loss: 1.4504830837249756\n",
            "Epoch: 187 Step: 20300 Val Loss: 1.447141408920288\n",
            "Epoch: 188 Step: 20325 Val Loss: 1.4500494003295898\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 188 Step: 20350 Val Loss: 1.4588619470596313\n",
            "Epoch: 188 Step: 20375 Val Loss: 1.4591995477676392\n",
            "Epoch: 188 Step: 20400 Val Loss: 1.4571287631988525\n",
            "Epoch: 189 Step: 20425 Val Loss: 1.4471291303634644\n",
            "Epoch: 189 Step: 20450 Val Loss: 1.450905680656433\n",
            "Epoch: 189 Step: 20475 Val Loss: 1.4527751207351685\n",
            "Epoch: 189 Step: 20500 Val Loss: 1.4578208923339844\n",
            "Epoch: 190 Step: 20525 Val Loss: 1.4458837509155273\n",
            "Epoch: 190 Step: 20550 Val Loss: 1.445646047592163\n",
            "Epoch: 190 Step: 20575 Val Loss: 1.450160026550293\n",
            "Epoch: 190 Step: 20600 Val Loss: 1.4798758029937744\n",
            "Epoch: 190 Step: 20625 Val Loss: 1.4517674446105957\n",
            "Epoch: 191 Step: 20650 Val Loss: 1.457478642463684\n",
            "Epoch: 191 Step: 20675 Val Loss: 1.4503834247589111\n",
            "Epoch: 191 Step: 20700 Val Loss: 1.4553642272949219\n",
            "Epoch: 191 Step: 20725 Val Loss: 1.4530929327011108\n",
            "Epoch: 192 Step: 20750 Val Loss: 1.4414950609207153\n",
            "Epoch: 192 Step: 20775 Val Loss: 1.4534295797348022\n",
            "Epoch: 192 Step: 20800 Val Loss: 1.451406478881836\n",
            "Epoch: 192 Step: 20825 Val Loss: 1.4526435136795044\n",
            "Epoch: 193 Step: 20850 Val Loss: 1.4458379745483398\n",
            "Epoch: 193 Step: 20875 Val Loss: 1.4520776271820068\n",
            "Epoch: 193 Step: 20900 Val Loss: 1.4586389064788818\n",
            "Epoch: 193 Step: 20925 Val Loss: 1.4560868740081787\n",
            "Epoch: 193 Step: 20950 Val Loss: 1.44895601272583\n",
            "Epoch: 194 Step: 20975 Val Loss: 1.4464952945709229\n",
            "Epoch: 194 Step: 21000 Val Loss: 1.4508004188537598\n",
            "Epoch: 194 Step: 21025 Val Loss: 1.4509453773498535\n",
            "Epoch: 194 Step: 21050 Val Loss: 1.4508552551269531\n",
            "Epoch: 195 Step: 21075 Val Loss: 1.4459304809570312\n",
            "Epoch: 195 Step: 21100 Val Loss: 1.450664758682251\n",
            "Epoch: 195 Step: 21125 Val Loss: 1.4523639678955078\n",
            "Epoch: 195 Step: 21150 Val Loss: 1.4515355825424194\n",
            "Epoch: 196 Step: 21175 Val Loss: 1.4466851949691772\n",
            "Epoch: 196 Step: 21200 Val Loss: 1.448107123374939\n",
            "Epoch: 196 Step: 21225 Val Loss: 1.451021671295166\n",
            "Epoch: 196 Step: 21250 Val Loss: 1.4499117136001587\n",
            "Epoch: 196 Step: 21275 Val Loss: 1.4460492134094238\n",
            "Epoch: 197 Step: 21300 Val Loss: 1.4498233795166016\n",
            "Epoch: 197 Step: 21325 Val Loss: 1.4533727169036865\n",
            "Epoch: 197 Step: 21350 Val Loss: 1.4518508911132812\n",
            "Epoch: 197 Step: 21375 Val Loss: 1.44646418094635\n",
            "Epoch: 198 Step: 21400 Val Loss: 1.4414492845535278\n",
            "Epoch: 198 Step: 21425 Val Loss: 1.447166919708252\n",
            "Epoch: 198 Step: 21450 Val Loss: 1.4470258951187134\n",
            "Epoch: 198 Step: 21475 Val Loss: 1.4554333686828613\n",
            "Epoch: 199 Step: 21500 Val Loss: 1.4418212175369263\n",
            "Epoch: 199 Step: 21525 Val Loss: 1.4473344087600708\n",
            "Epoch: 199 Step: 21550 Val Loss: 1.4517858028411865\n",
            "Epoch: 199 Step: 21575 Val Loss: 1.4483301639556885\n",
            "Epoch: 199 Step: 21600 Val Loss: 1.4440221786499023\n",
            "Epoch: 200 Step: 21625 Val Loss: 1.4473741054534912\n",
            "Epoch: 200 Step: 21650 Val Loss: 1.4509029388427734\n",
            "Epoch: 200 Step: 21675 Val Loss: 1.4569625854492188\n",
            "Epoch: 200 Step: 21700 Val Loss: 1.4454885721206665\n",
            "Epoch: 201 Step: 21725 Val Loss: 1.440184235572815\n",
            "Epoch: 201 Step: 21750 Val Loss: 1.4466838836669922\n",
            "Epoch: 201 Step: 21775 Val Loss: 1.4473015069961548\n",
            "Epoch: 201 Step: 21800 Val Loss: 1.4518184661865234\n",
            "Epoch: 202 Step: 21825 Val Loss: 1.4438287019729614\n",
            "Epoch: 202 Step: 21850 Val Loss: 1.4442880153656006\n",
            "Epoch: 202 Step: 21875 Val Loss: 1.4490587711334229\n",
            "Epoch: 202 Step: 21900 Val Loss: 1.4550710916519165\n",
            "Epoch: 203 Step: 21925 Val Loss: 1.4469257593154907\n",
            "Epoch: 203 Step: 21950 Val Loss: 1.4472119808197021\n",
            "Epoch: 203 Step: 21975 Val Loss: 1.4574337005615234\n",
            "Epoch: 203 Step: 22000 Val Loss: 1.4560279846191406\n",
            "Epoch: 203 Step: 22025 Val Loss: 1.4499225616455078\n",
            "Epoch: 204 Step: 22050 Val Loss: 1.4452495574951172\n",
            "Epoch: 204 Step: 22075 Val Loss: 1.45025634765625\n",
            "Epoch: 204 Step: 22100 Val Loss: 1.4534088373184204\n",
            "Epoch: 204 Step: 22125 Val Loss: 1.453867793083191\n",
            "Epoch: 205 Step: 22150 Val Loss: 1.4408694505691528\n",
            "Epoch: 205 Step: 22175 Val Loss: 1.4464929103851318\n",
            "Epoch: 205 Step: 22200 Val Loss: 1.4479676485061646\n",
            "Epoch: 205 Step: 22225 Val Loss: 1.4500088691711426\n",
            "Epoch: 206 Step: 22250 Val Loss: 1.4416537284851074\n",
            "Epoch: 206 Step: 22275 Val Loss: 1.4438525438308716\n",
            "Epoch: 206 Step: 22300 Val Loss: 1.447261929512024\n",
            "Epoch: 206 Step: 22325 Val Loss: 1.4529181718826294\n",
            "Epoch: 206 Step: 22350 Val Loss: 1.4430701732635498\n",
            "Epoch: 207 Step: 22375 Val Loss: 1.444391131401062\n",
            "Epoch: 207 Step: 22400 Val Loss: 1.4557812213897705\n",
            "Epoch: 207 Step: 22425 Val Loss: 1.4555087089538574\n",
            "Epoch: 207 Step: 22450 Val Loss: 1.4536857604980469\n",
            "Epoch: 208 Step: 22475 Val Loss: 1.4407492876052856\n",
            "Epoch: 208 Step: 22500 Val Loss: 1.4515715837478638\n",
            "Epoch: 208 Step: 22525 Val Loss: 1.4511737823486328\n",
            "Epoch: 208 Step: 22550 Val Loss: 1.451151967048645\n",
            "Epoch: 209 Step: 22575 Val Loss: 1.4412225484848022\n",
            "Epoch: 209 Step: 22600 Val Loss: 1.444183588027954\n",
            "Epoch: 209 Step: 22625 Val Loss: 1.4490532875061035\n",
            "Epoch: 209 Step: 22650 Val Loss: 1.4499551057815552\n",
            "Epoch: 209 Step: 22675 Val Loss: 1.4431849718093872\n",
            "Epoch: 210 Step: 22700 Val Loss: 1.4414302110671997\n",
            "Epoch: 210 Step: 22725 Val Loss: 1.447157859802246\n",
            "Epoch: 210 Step: 22750 Val Loss: 1.4477169513702393\n",
            "Epoch: 210 Step: 22775 Val Loss: 1.450822114944458\n",
            "Epoch: 211 Step: 22800 Val Loss: 1.440999984741211\n",
            "Epoch: 211 Step: 22825 Val Loss: 1.4445399045944214\n",
            "Epoch: 211 Step: 22850 Val Loss: 1.4462151527404785\n",
            "Epoch: 211 Step: 22875 Val Loss: 1.454368233680725\n",
            "Epoch: 212 Step: 22900 Val Loss: 1.4426418542861938\n",
            "Epoch: 212 Step: 22925 Val Loss: 1.4435714483261108\n",
            "Epoch: 212 Step: 22950 Val Loss: 1.4452483654022217\n",
            "Epoch: 212 Step: 22975 Val Loss: 1.450228214263916\n",
            "Epoch: 212 Step: 23000 Val Loss: 1.4439948797225952\n",
            "Epoch: 213 Step: 23025 Val Loss: 1.439066767692566\n",
            "Epoch: 213 Step: 23050 Val Loss: 1.4434964656829834\n",
            "Epoch: 213 Step: 23075 Val Loss: 1.4442037343978882\n",
            "Epoch: 213 Step: 23100 Val Loss: 1.4387545585632324\n",
            "Epoch: 214 Step: 23125 Val Loss: 1.4356716871261597\n",
            "Epoch: 214 Step: 23150 Val Loss: 1.4453394412994385\n",
            "Epoch: 214 Step: 23175 Val Loss: 1.4458132982254028\n",
            "Epoch: 214 Step: 23200 Val Loss: 1.4490429162979126\n",
            "Epoch: 215 Step: 23225 Val Loss: 1.4365450143814087\n",
            "Epoch: 215 Step: 23250 Val Loss: 1.436431884765625\n",
            "Epoch: 215 Step: 23275 Val Loss: 1.436552882194519\n",
            "Epoch: 215 Step: 23300 Val Loss: 1.4473845958709717\n",
            "Epoch: 215 Step: 23325 Val Loss: 1.4409362077713013\n",
            "Epoch: 216 Step: 23350 Val Loss: 1.4367780685424805\n",
            "Epoch: 216 Step: 23375 Val Loss: 1.4427696466445923\n",
            "Epoch: 216 Step: 23400 Val Loss: 1.4479247331619263\n",
            "Epoch: 216 Step: 23425 Val Loss: 1.4403932094573975\n",
            "Epoch: 217 Step: 23450 Val Loss: 1.4322482347488403\n",
            "Epoch: 217 Step: 23475 Val Loss: 1.4422199726104736\n",
            "Epoch: 217 Step: 23500 Val Loss: 1.4400756359100342\n",
            "Epoch: 217 Step: 23525 Val Loss: 1.4435961246490479\n",
            "Epoch: 218 Step: 23550 Val Loss: 1.4342187643051147\n",
            "Epoch: 218 Step: 23575 Val Loss: 1.4387983083724976\n",
            "Epoch: 218 Step: 23600 Val Loss: 1.443311095237732\n",
            "Epoch: 218 Step: 23625 Val Loss: 1.4446367025375366\n",
            "Epoch: 218 Step: 23650 Val Loss: 1.434423804283142\n",
            "Epoch: 219 Step: 23675 Val Loss: 1.4348090887069702\n",
            "Epoch: 219 Step: 23700 Val Loss: 1.4453455209732056\n",
            "Epoch: 219 Step: 23725 Val Loss: 1.4373441934585571\n",
            "Epoch: 219 Step: 23750 Val Loss: 1.436035394668579\n",
            "Epoch: 220 Step: 23775 Val Loss: 1.4350157976150513\n",
            "Epoch: 220 Step: 23800 Val Loss: 1.4398711919784546\n",
            "Epoch: 220 Step: 23825 Val Loss: 1.4426206350326538\n",
            "Epoch: 220 Step: 23850 Val Loss: 1.4427263736724854\n",
            "Epoch: 221 Step: 23875 Val Loss: 1.4323339462280273\n",
            "Epoch: 221 Step: 23900 Val Loss: 1.4353585243225098\n",
            "Epoch: 221 Step: 23925 Val Loss: 1.436951756477356\n",
            "Epoch: 221 Step: 23950 Val Loss: 1.4365332126617432\n",
            "Epoch: 221 Step: 23975 Val Loss: 1.4381377696990967\n",
            "Epoch: 222 Step: 24000 Val Loss: 1.4346073865890503\n",
            "Epoch: 222 Step: 24025 Val Loss: 1.4402050971984863\n",
            "Epoch: 222 Step: 24050 Val Loss: 1.4401016235351562\n",
            "Epoch: 222 Step: 24075 Val Loss: 1.4364800453186035\n",
            "Epoch: 223 Step: 24100 Val Loss: 1.4328469038009644\n",
            "Epoch: 223 Step: 24125 Val Loss: 1.4421005249023438\n",
            "Epoch: 223 Step: 24150 Val Loss: 1.4442263841629028\n",
            "Epoch: 223 Step: 24175 Val Loss: 1.4474560022354126\n",
            "Epoch: 224 Step: 24200 Val Loss: 1.4365259408950806\n",
            "Epoch: 224 Step: 24225 Val Loss: 1.4406487941741943\n",
            "Epoch: 224 Step: 24250 Val Loss: 1.4402128458023071\n",
            "Epoch: 224 Step: 24275 Val Loss: 1.4398565292358398\n",
            "Epoch: 224 Step: 24300 Val Loss: 1.4335792064666748\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 225 Step: 24325 Val Loss: 1.4373377561569214\n",
            "Epoch: 225 Step: 24350 Val Loss: 1.4431836605072021\n",
            "Epoch: 225 Step: 24375 Val Loss: 1.4360252618789673\n",
            "Epoch: 225 Step: 24400 Val Loss: 1.4348260164260864\n",
            "Epoch: 226 Step: 24425 Val Loss: 1.4328052997589111\n",
            "Epoch: 226 Step: 24450 Val Loss: 1.4400509595870972\n",
            "Epoch: 226 Step: 24475 Val Loss: 1.4374102354049683\n",
            "Epoch: 226 Step: 24500 Val Loss: 1.4420114755630493\n",
            "Epoch: 227 Step: 24525 Val Loss: 1.4337427616119385\n",
            "Epoch: 227 Step: 24550 Val Loss: 1.437562108039856\n",
            "Epoch: 227 Step: 24575 Val Loss: 1.4428538084030151\n",
            "Epoch: 227 Step: 24600 Val Loss: 1.4411636590957642\n",
            "Epoch: 228 Step: 24625 Val Loss: 1.4340211153030396\n",
            "Epoch: 228 Step: 24650 Val Loss: 1.4325264692306519\n",
            "Epoch: 228 Step: 24675 Val Loss: 1.4347875118255615\n",
            "Epoch: 228 Step: 24700 Val Loss: 1.4377899169921875\n",
            "Epoch: 228 Step: 24725 Val Loss: 1.4345853328704834\n",
            "Epoch: 229 Step: 24750 Val Loss: 1.4306416511535645\n",
            "Epoch: 229 Step: 24775 Val Loss: 1.4357877969741821\n",
            "Epoch: 229 Step: 24800 Val Loss: 1.4345089197158813\n",
            "Epoch: 229 Step: 24825 Val Loss: 1.4366387128829956\n",
            "Epoch: 230 Step: 24850 Val Loss: 1.432607650756836\n",
            "Epoch: 230 Step: 24875 Val Loss: 1.439913272857666\n",
            "Epoch: 230 Step: 24900 Val Loss: 1.445190668106079\n",
            "Epoch: 230 Step: 24925 Val Loss: 1.4398808479309082\n",
            "Epoch: 231 Step: 24950 Val Loss: 1.4316790103912354\n",
            "Epoch: 231 Step: 24975 Val Loss: 1.4363471269607544\n",
            "Epoch: 231 Step: 25000 Val Loss: 1.4445538520812988\n",
            "Epoch: 231 Step: 25025 Val Loss: 1.4424587488174438\n",
            "Epoch: 231 Step: 25050 Val Loss: 1.4383840560913086\n",
            "Epoch: 232 Step: 25075 Val Loss: 1.4330860376358032\n",
            "Epoch: 232 Step: 25100 Val Loss: 1.4358882904052734\n",
            "Epoch: 232 Step: 25125 Val Loss: 1.4420583248138428\n",
            "Epoch: 232 Step: 25150 Val Loss: 1.4390106201171875\n",
            "Epoch: 233 Step: 25175 Val Loss: 1.4321702718734741\n",
            "Epoch: 233 Step: 25200 Val Loss: 1.442973256111145\n",
            "Epoch: 233 Step: 25225 Val Loss: 1.44133722782135\n",
            "Epoch: 233 Step: 25250 Val Loss: 1.442339301109314\n",
            "Epoch: 234 Step: 25275 Val Loss: 1.432658076286316\n",
            "Epoch: 234 Step: 25300 Val Loss: 1.436289668083191\n",
            "Epoch: 234 Step: 25325 Val Loss: 1.4441457986831665\n",
            "Epoch: 234 Step: 25350 Val Loss: 1.439367651939392\n",
            "Epoch: 234 Step: 25375 Val Loss: 1.4394129514694214\n",
            "Epoch: 235 Step: 25400 Val Loss: 1.4335286617279053\n",
            "Epoch: 235 Step: 25425 Val Loss: 1.435712218284607\n",
            "Epoch: 235 Step: 25450 Val Loss: 1.4310379028320312\n",
            "Epoch: 235 Step: 25475 Val Loss: 1.4327988624572754\n",
            "Epoch: 236 Step: 25500 Val Loss: 1.4272046089172363\n",
            "Epoch: 236 Step: 25525 Val Loss: 1.4357317686080933\n",
            "Epoch: 236 Step: 25550 Val Loss: 1.4397432804107666\n",
            "Epoch: 236 Step: 25575 Val Loss: 1.439955234527588\n",
            "Epoch: 237 Step: 25600 Val Loss: 1.4306118488311768\n",
            "Epoch: 237 Step: 25625 Val Loss: 1.4305394887924194\n",
            "Epoch: 237 Step: 25650 Val Loss: 1.4350582361221313\n",
            "Epoch: 237 Step: 25675 Val Loss: 1.4326714277267456\n",
            "Epoch: 237 Step: 25700 Val Loss: 1.4307045936584473\n",
            "Epoch: 238 Step: 25725 Val Loss: 1.429499626159668\n",
            "Epoch: 238 Step: 25750 Val Loss: 1.4366883039474487\n",
            "Epoch: 238 Step: 25775 Val Loss: 1.4394587278366089\n",
            "Epoch: 238 Step: 25800 Val Loss: 1.437778115272522\n",
            "Epoch: 239 Step: 25825 Val Loss: 1.4290670156478882\n",
            "Epoch: 239 Step: 25850 Val Loss: 1.4336297512054443\n",
            "Epoch: 239 Step: 25875 Val Loss: 1.4317529201507568\n",
            "Epoch: 239 Step: 25900 Val Loss: 1.4354923963546753\n",
            "Epoch: 240 Step: 25925 Val Loss: 1.4293386936187744\n",
            "Epoch: 240 Step: 25950 Val Loss: 1.4343295097351074\n",
            "Epoch: 240 Step: 25975 Val Loss: 1.4388952255249023\n",
            "Epoch: 240 Step: 26000 Val Loss: 1.4387249946594238\n",
            "Epoch: 240 Step: 26025 Val Loss: 1.4357351064682007\n",
            "Epoch: 241 Step: 26050 Val Loss: 1.4308005571365356\n",
            "Epoch: 241 Step: 26075 Val Loss: 1.4334759712219238\n",
            "Epoch: 241 Step: 26100 Val Loss: 1.4361261129379272\n",
            "Epoch: 241 Step: 26125 Val Loss: 1.4345991611480713\n",
            "Epoch: 242 Step: 26150 Val Loss: 1.4246351718902588\n",
            "Epoch: 242 Step: 26175 Val Loss: 1.429223895072937\n",
            "Epoch: 242 Step: 26200 Val Loss: 1.4313149452209473\n",
            "Epoch: 242 Step: 26225 Val Loss: 1.4382095336914062\n",
            "Epoch: 243 Step: 26250 Val Loss: 1.4275041818618774\n",
            "Epoch: 243 Step: 26275 Val Loss: 1.4385236501693726\n",
            "Epoch: 243 Step: 26300 Val Loss: 1.434181809425354\n",
            "Epoch: 243 Step: 26325 Val Loss: 1.4331071376800537\n",
            "Epoch: 243 Step: 26350 Val Loss: 1.4278570413589478\n",
            "Epoch: 244 Step: 26375 Val Loss: 1.428263545036316\n",
            "Epoch: 244 Step: 26400 Val Loss: 1.4391953945159912\n",
            "Epoch: 244 Step: 26425 Val Loss: 1.4375278949737549\n",
            "Epoch: 244 Step: 26450 Val Loss: 1.4456969499588013\n",
            "Epoch: 245 Step: 26475 Val Loss: 1.4307836294174194\n",
            "Epoch: 245 Step: 26500 Val Loss: 1.4385405778884888\n",
            "Epoch: 245 Step: 26525 Val Loss: 1.4336105585098267\n",
            "Epoch: 245 Step: 26550 Val Loss: 1.4447296857833862\n",
            "Epoch: 246 Step: 26575 Val Loss: 1.4316134452819824\n",
            "Epoch: 246 Step: 26600 Val Loss: 1.4326893091201782\n",
            "Epoch: 246 Step: 26625 Val Loss: 1.434477686882019\n",
            "Epoch: 246 Step: 26650 Val Loss: 1.4352576732635498\n",
            "Epoch: 246 Step: 26675 Val Loss: 1.4303927421569824\n",
            "Epoch: 247 Step: 26700 Val Loss: 1.43077552318573\n",
            "Epoch: 247 Step: 26725 Val Loss: 1.436562418937683\n",
            "Epoch: 247 Step: 26750 Val Loss: 1.4336111545562744\n",
            "Epoch: 247 Step: 26775 Val Loss: 1.43364679813385\n",
            "Epoch: 248 Step: 26800 Val Loss: 1.42900550365448\n",
            "Epoch: 248 Step: 26825 Val Loss: 1.4309611320495605\n",
            "Epoch: 248 Step: 26850 Val Loss: 1.4300506114959717\n",
            "Epoch: 248 Step: 26875 Val Loss: 1.433850646018982\n",
            "Epoch: 249 Step: 26900 Val Loss: 1.4315145015716553\n",
            "Epoch: 249 Step: 26925 Val Loss: 1.431599736213684\n",
            "Epoch: 249 Step: 26950 Val Loss: 1.432457447052002\n",
            "Epoch: 249 Step: 26975 Val Loss: 1.4377350807189941\n",
            "Epoch: 249 Step: 27000 Val Loss: 1.4335269927978516\n",
            "Epoch: 250 Step: 27025 Val Loss: 1.4330220222473145\n",
            "Epoch: 250 Step: 27050 Val Loss: 1.442474365234375\n",
            "Epoch: 250 Step: 27075 Val Loss: 1.4361507892608643\n",
            "Epoch: 250 Step: 27100 Val Loss: 1.4309086799621582\n",
            "Epoch: 251 Step: 27125 Val Loss: 1.4251890182495117\n",
            "Epoch: 251 Step: 27150 Val Loss: 1.4295029640197754\n",
            "Epoch: 251 Step: 27175 Val Loss: 1.4307061433792114\n",
            "Epoch: 251 Step: 27200 Val Loss: 1.4338675737380981\n",
            "Epoch: 252 Step: 27225 Val Loss: 1.4258952140808105\n",
            "Epoch: 252 Step: 27250 Val Loss: 1.430139422416687\n",
            "Epoch: 252 Step: 27275 Val Loss: 1.4298021793365479\n",
            "Epoch: 252 Step: 27300 Val Loss: 1.4286763668060303\n",
            "Epoch: 253 Step: 27325 Val Loss: 1.4252679347991943\n",
            "Epoch: 253 Step: 27350 Val Loss: 1.4233944416046143\n",
            "Epoch: 253 Step: 27375 Val Loss: 1.4315030574798584\n",
            "Epoch: 253 Step: 27400 Val Loss: 1.4324300289154053\n",
            "Epoch: 253 Step: 27425 Val Loss: 1.4278122186660767\n",
            "Epoch: 254 Step: 27450 Val Loss: 1.425176978111267\n",
            "Epoch: 254 Step: 27475 Val Loss: 1.4298969507217407\n",
            "Epoch: 254 Step: 27500 Val Loss: 1.4297229051589966\n",
            "Epoch: 254 Step: 27525 Val Loss: 1.4329988956451416\n",
            "Epoch: 255 Step: 27550 Val Loss: 1.4239346981048584\n",
            "Epoch: 255 Step: 27575 Val Loss: 1.4289343357086182\n",
            "Epoch: 255 Step: 27600 Val Loss: 1.4301866292953491\n",
            "Epoch: 255 Step: 27625 Val Loss: 1.4274433851242065\n",
            "Epoch: 256 Step: 27650 Val Loss: 1.4226021766662598\n",
            "Epoch: 256 Step: 27675 Val Loss: 1.4242957830429077\n",
            "Epoch: 256 Step: 27700 Val Loss: 1.4289803504943848\n",
            "Epoch: 256 Step: 27725 Val Loss: 1.4292963743209839\n",
            "Epoch: 256 Step: 27750 Val Loss: 1.4233534336090088\n",
            "Epoch: 257 Step: 27775 Val Loss: 1.423403263092041\n",
            "Epoch: 257 Step: 27800 Val Loss: 1.4311888217926025\n",
            "Epoch: 257 Step: 27825 Val Loss: 1.4312386512756348\n",
            "Epoch: 257 Step: 27850 Val Loss: 1.4307540655136108\n",
            "Epoch: 258 Step: 27875 Val Loss: 1.4255917072296143\n",
            "Epoch: 258 Step: 27900 Val Loss: 1.4665913581848145\n",
            "Epoch: 258 Step: 27925 Val Loss: 1.4406222105026245\n",
            "Epoch: 258 Step: 27950 Val Loss: 1.4392225742340088\n",
            "Epoch: 259 Step: 27975 Val Loss: 1.4270974397659302\n",
            "Epoch: 259 Step: 28000 Val Loss: 1.4272619485855103\n",
            "Epoch: 259 Step: 28025 Val Loss: 1.4328316450119019\n",
            "Epoch: 259 Step: 28050 Val Loss: 1.4306179285049438\n",
            "Epoch: 259 Step: 28075 Val Loss: 1.4257863759994507\n",
            "Epoch: 260 Step: 28100 Val Loss: 1.4233466386795044\n",
            "Epoch: 260 Step: 28125 Val Loss: 1.4261828660964966\n",
            "Epoch: 260 Step: 28150 Val Loss: 1.4268702268600464\n",
            "Epoch: 260 Step: 28175 Val Loss: 1.4276379346847534\n",
            "Epoch: 261 Step: 28200 Val Loss: 1.4231020212173462\n",
            "Epoch: 261 Step: 28225 Val Loss: 1.4233605861663818\n",
            "Epoch: 261 Step: 28250 Val Loss: 1.4287917613983154\n",
            "Epoch: 261 Step: 28275 Val Loss: 1.4319595098495483\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 262 Step: 28300 Val Loss: 1.4232409000396729\n",
            "Epoch: 262 Step: 28325 Val Loss: 1.4217318296432495\n",
            "Epoch: 262 Step: 28350 Val Loss: 1.4196670055389404\n",
            "Epoch: 262 Step: 28375 Val Loss: 1.421472430229187\n",
            "Epoch: 262 Step: 28400 Val Loss: 1.420918345451355\n",
            "Epoch: 263 Step: 28425 Val Loss: 1.4202375411987305\n",
            "Epoch: 263 Step: 28450 Val Loss: 1.4308359622955322\n",
            "Epoch: 263 Step: 28475 Val Loss: 1.4305953979492188\n",
            "Epoch: 263 Step: 28500 Val Loss: 1.4275624752044678\n",
            "Epoch: 264 Step: 28525 Val Loss: 1.422402262687683\n",
            "Epoch: 264 Step: 28550 Val Loss: 1.4248110055923462\n",
            "Epoch: 264 Step: 28575 Val Loss: 1.4241697788238525\n",
            "Epoch: 264 Step: 28600 Val Loss: 1.4259803295135498\n",
            "Epoch: 265 Step: 28625 Val Loss: 1.4213019609451294\n",
            "Epoch: 265 Step: 28650 Val Loss: 1.4231029748916626\n",
            "Epoch: 265 Step: 28675 Val Loss: 1.425592064857483\n",
            "Epoch: 265 Step: 28700 Val Loss: 1.425460934638977\n",
            "Epoch: 265 Step: 28725 Val Loss: 1.4206184148788452\n",
            "Epoch: 266 Step: 28750 Val Loss: 1.4211667776107788\n",
            "Epoch: 266 Step: 28775 Val Loss: 1.42070734500885\n",
            "Epoch: 266 Step: 28800 Val Loss: 1.4206987619400024\n",
            "Epoch: 266 Step: 28825 Val Loss: 1.4214134216308594\n",
            "Epoch: 267 Step: 28850 Val Loss: 1.4161486625671387\n",
            "Epoch: 267 Step: 28875 Val Loss: 1.4196425676345825\n",
            "Epoch: 267 Step: 28900 Val Loss: 1.4204349517822266\n",
            "Epoch: 267 Step: 28925 Val Loss: 1.4212932586669922\n",
            "Epoch: 268 Step: 28950 Val Loss: 1.4199243783950806\n",
            "Epoch: 268 Step: 28975 Val Loss: 1.4172130823135376\n",
            "Epoch: 268 Step: 29000 Val Loss: 1.4183197021484375\n",
            "Epoch: 268 Step: 29025 Val Loss: 1.4201253652572632\n",
            "Epoch: 268 Step: 29050 Val Loss: 1.4198795557022095\n",
            "Epoch: 269 Step: 29075 Val Loss: 1.417862057685852\n",
            "Epoch: 269 Step: 29100 Val Loss: 1.4187209606170654\n",
            "Epoch: 269 Step: 29125 Val Loss: 1.419167399406433\n",
            "Epoch: 269 Step: 29150 Val Loss: 1.420129656791687\n",
            "Epoch: 270 Step: 29175 Val Loss: 1.4178870916366577\n",
            "Epoch: 270 Step: 29200 Val Loss: 1.4185967445373535\n",
            "Epoch: 270 Step: 29225 Val Loss: 1.4183883666992188\n",
            "Epoch: 270 Step: 29250 Val Loss: 1.4198511838912964\n",
            "Epoch: 271 Step: 29275 Val Loss: 1.415899634361267\n",
            "Epoch: 271 Step: 29300 Val Loss: 1.4173662662506104\n",
            "Epoch: 271 Step: 29325 Val Loss: 1.4213590621948242\n",
            "Epoch: 271 Step: 29350 Val Loss: 1.4208526611328125\n",
            "Epoch: 271 Step: 29375 Val Loss: 1.4153286218643188\n",
            "Epoch: 272 Step: 29400 Val Loss: 1.4164576530456543\n",
            "Epoch: 272 Step: 29425 Val Loss: 1.4192428588867188\n",
            "Epoch: 272 Step: 29450 Val Loss: 1.430074691772461\n",
            "Epoch: 272 Step: 29475 Val Loss: 1.4194313287734985\n",
            "Epoch: 273 Step: 29500 Val Loss: 1.4153330326080322\n",
            "Epoch: 273 Step: 29525 Val Loss: 1.4215481281280518\n",
            "Epoch: 273 Step: 29550 Val Loss: 1.4214985370635986\n",
            "Epoch: 273 Step: 29575 Val Loss: 1.422935128211975\n",
            "Epoch: 274 Step: 29600 Val Loss: 1.4174717664718628\n",
            "Epoch: 274 Step: 29625 Val Loss: 1.4207043647766113\n",
            "Epoch: 274 Step: 29650 Val Loss: 1.4203743934631348\n",
            "Epoch: 274 Step: 29675 Val Loss: 1.4193121194839478\n",
            "Epoch: 274 Step: 29700 Val Loss: 1.4162533283233643\n",
            "Epoch: 275 Step: 29725 Val Loss: 1.4185951948165894\n",
            "Epoch: 275 Step: 29750 Val Loss: 1.420459508895874\n",
            "Epoch: 275 Step: 29775 Val Loss: 1.4198096990585327\n",
            "Epoch: 275 Step: 29800 Val Loss: 1.4190466403961182\n",
            "Epoch: 276 Step: 29825 Val Loss: 1.4183224439620972\n",
            "Epoch: 276 Step: 29850 Val Loss: 1.423986792564392\n",
            "Epoch: 276 Step: 29875 Val Loss: 1.4196075201034546\n",
            "Epoch: 276 Step: 29900 Val Loss: 1.4207779169082642\n",
            "Epoch: 277 Step: 29925 Val Loss: 1.4172526597976685\n",
            "Epoch: 277 Step: 29950 Val Loss: 1.4194347858428955\n",
            "Epoch: 277 Step: 29975 Val Loss: 1.4215362071990967\n",
            "Epoch: 277 Step: 30000 Val Loss: 1.4193115234375\n",
            "Epoch: 278 Step: 30025 Val Loss: 1.4146333932876587\n",
            "Epoch: 278 Step: 30050 Val Loss: 1.418681025505066\n",
            "Epoch: 278 Step: 30075 Val Loss: 1.4206842184066772\n",
            "Epoch: 278 Step: 30100 Val Loss: 1.4190256595611572\n",
            "Epoch: 278 Step: 30125 Val Loss: 1.4183508157730103\n",
            "Epoch: 279 Step: 30150 Val Loss: 1.4174926280975342\n",
            "Epoch: 279 Step: 30175 Val Loss: 1.4206839799880981\n",
            "Epoch: 279 Step: 30200 Val Loss: 1.4169061183929443\n",
            "Epoch: 279 Step: 30225 Val Loss: 1.415228247642517\n",
            "Epoch: 280 Step: 30250 Val Loss: 1.4146264791488647\n",
            "Epoch: 280 Step: 30275 Val Loss: 1.419426679611206\n",
            "Epoch: 280 Step: 30300 Val Loss: 1.4192782640457153\n",
            "Epoch: 280 Step: 30325 Val Loss: 1.4241083860397339\n",
            "Epoch: 281 Step: 30350 Val Loss: 1.4163360595703125\n",
            "Epoch: 281 Step: 30375 Val Loss: 1.42019522190094\n",
            "Epoch: 281 Step: 30400 Val Loss: 1.4228686094284058\n",
            "Epoch: 281 Step: 30425 Val Loss: 1.4221171140670776\n",
            "Epoch: 281 Step: 30450 Val Loss: 1.4174317121505737\n",
            "Epoch: 282 Step: 30475 Val Loss: 1.4155099391937256\n",
            "Epoch: 282 Step: 30500 Val Loss: 1.419605016708374\n",
            "Epoch: 282 Step: 30525 Val Loss: 1.415812611579895\n",
            "Epoch: 282 Step: 30550 Val Loss: 1.4221032857894897\n",
            "Epoch: 283 Step: 30575 Val Loss: 1.4163286685943604\n",
            "Epoch: 283 Step: 30600 Val Loss: 1.417466402053833\n",
            "Epoch: 283 Step: 30625 Val Loss: 1.4151169061660767\n",
            "Epoch: 283 Step: 30650 Val Loss: 1.4134222269058228\n",
            "Epoch: 284 Step: 30675 Val Loss: 1.413329839706421\n",
            "Epoch: 284 Step: 30700 Val Loss: 1.414825439453125\n",
            "Epoch: 284 Step: 30725 Val Loss: 1.4198641777038574\n",
            "Epoch: 284 Step: 30750 Val Loss: 1.4189181327819824\n",
            "Epoch: 284 Step: 30775 Val Loss: 1.4155056476593018\n",
            "Epoch: 285 Step: 30800 Val Loss: 1.4157363176345825\n",
            "Epoch: 285 Step: 30825 Val Loss: 1.4185280799865723\n",
            "Epoch: 285 Step: 30850 Val Loss: 1.4174953699111938\n",
            "Epoch: 285 Step: 30875 Val Loss: 1.416917085647583\n",
            "Epoch: 286 Step: 30900 Val Loss: 1.4139559268951416\n",
            "Epoch: 286 Step: 30925 Val Loss: 1.4207427501678467\n",
            "Epoch: 286 Step: 30950 Val Loss: 1.4211453199386597\n",
            "Epoch: 286 Step: 30975 Val Loss: 1.4204355478286743\n",
            "Epoch: 287 Step: 31000 Val Loss: 1.4150856733322144\n",
            "Epoch: 287 Step: 31025 Val Loss: 1.4183309078216553\n",
            "Epoch: 287 Step: 31050 Val Loss: 1.4228571653366089\n",
            "Epoch: 287 Step: 31075 Val Loss: 1.4193458557128906\n",
            "Epoch: 287 Step: 31100 Val Loss: 1.4132407903671265\n",
            "Epoch: 288 Step: 31125 Val Loss: 1.4150232076644897\n",
            "Epoch: 288 Step: 31150 Val Loss: 1.4162745475769043\n",
            "Epoch: 288 Step: 31175 Val Loss: 1.4184426069259644\n",
            "Epoch: 288 Step: 31200 Val Loss: 1.416595458984375\n",
            "Epoch: 289 Step: 31225 Val Loss: 1.4155434370040894\n",
            "Epoch: 289 Step: 31250 Val Loss: 1.4183619022369385\n",
            "Epoch: 289 Step: 31275 Val Loss: 1.4179143905639648\n",
            "Epoch: 289 Step: 31300 Val Loss: 1.423388957977295\n",
            "Epoch: 290 Step: 31325 Val Loss: 1.4147192239761353\n",
            "Epoch: 290 Step: 31350 Val Loss: 1.4153995513916016\n",
            "Epoch: 290 Step: 31375 Val Loss: 1.4160202741622925\n",
            "Epoch: 290 Step: 31400 Val Loss: 1.4193856716156006\n",
            "Epoch: 290 Step: 31425 Val Loss: 1.4178768396377563\n",
            "Epoch: 291 Step: 31450 Val Loss: 1.4136738777160645\n",
            "Epoch: 291 Step: 31475 Val Loss: 1.4184659719467163\n",
            "Epoch: 291 Step: 31500 Val Loss: 1.4159992933273315\n",
            "Epoch: 291 Step: 31525 Val Loss: 1.4164425134658813\n",
            "Epoch: 292 Step: 31550 Val Loss: 1.4122074842453003\n",
            "Epoch: 292 Step: 31575 Val Loss: 1.4169021844863892\n",
            "Epoch: 292 Step: 31600 Val Loss: 1.41524076461792\n",
            "Epoch: 292 Step: 31625 Val Loss: 1.4156615734100342\n",
            "Epoch: 293 Step: 31650 Val Loss: 1.4129821062088013\n",
            "Epoch: 293 Step: 31675 Val Loss: 1.4145427942276\n",
            "Epoch: 293 Step: 31700 Val Loss: 1.419634461402893\n",
            "Epoch: 293 Step: 31725 Val Loss: 1.4191848039627075\n",
            "Epoch: 293 Step: 31750 Val Loss: 1.4138739109039307\n",
            "Epoch: 294 Step: 31775 Val Loss: 1.4117995500564575\n",
            "Epoch: 294 Step: 31800 Val Loss: 1.416709542274475\n",
            "Epoch: 294 Step: 31825 Val Loss: 1.4141689538955688\n",
            "Epoch: 294 Step: 31850 Val Loss: 1.416399359703064\n",
            "Epoch: 295 Step: 31875 Val Loss: 1.4156975746154785\n",
            "Epoch: 295 Step: 31900 Val Loss: 1.4173223972320557\n",
            "Epoch: 295 Step: 31925 Val Loss: 1.4177815914154053\n",
            "Epoch: 295 Step: 31950 Val Loss: 1.4212572574615479\n",
            "Epoch: 296 Step: 31975 Val Loss: 1.4155546426773071\n",
            "Epoch: 296 Step: 32000 Val Loss: 1.416772723197937\n",
            "Epoch: 296 Step: 32025 Val Loss: 1.4179799556732178\n",
            "Epoch: 296 Step: 32050 Val Loss: 1.4174001216888428\n",
            "Epoch: 296 Step: 32075 Val Loss: 1.4140065908432007\n",
            "Epoch: 297 Step: 32100 Val Loss: 1.4140642881393433\n",
            "Epoch: 297 Step: 32125 Val Loss: 1.4179043769836426\n",
            "Epoch: 297 Step: 32150 Val Loss: 1.4136524200439453\n",
            "Epoch: 297 Step: 32175 Val Loss: 1.4110684394836426\n",
            "Epoch: 298 Step: 32200 Val Loss: 1.410277009010315\n",
            "Epoch: 298 Step: 32225 Val Loss: 1.4150426387786865\n",
            "Epoch: 298 Step: 32250 Val Loss: 1.4183673858642578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 298 Step: 32275 Val Loss: 1.420920968055725\n",
            "Epoch: 299 Step: 32300 Val Loss: 1.4159923791885376\n",
            "Epoch: 299 Step: 32325 Val Loss: 1.415917992591858\n",
            "Epoch: 299 Step: 32350 Val Loss: 1.4138621091842651\n",
            "Epoch: 299 Step: 32375 Val Loss: 1.4156651496887207\n",
            "Epoch: 299 Step: 32400 Val Loss: 1.4144048690795898\n"
          ]
        }
      ],
      "source": [
        "epochs_size = 300\n",
        "sequation_lenght = 20\n",
        "batch_size = 5\n",
        "step = 0\n",
        "\n",
        "# Set model to train\n",
        "model.train()\n",
        "if model.use_gpu:\n",
        "    model = model.cuda()\n",
        "\n",
        "for i in range(epochs_size):    \n",
        "    hidden = model.hidden_state(batch_size)\n",
        "    for x,y in batch(train_dataset,batch_size,sequation_lenght):\n",
        "        \n",
        "        step += 1\n",
        "        \n",
        "        # One Hot Encode incoming data\n",
        "        x = transform(model,x)\n",
        "        y = transform(model,y)\n",
        "        # Convert Numpy Arrays to Tensor\n",
        "        \n",
        "        inputs = torch.from_numpy(x).float()\n",
        "        targets = torch.from_numpy(y).float()\n",
        "               \n",
        "        if model.use_gpu:\n",
        "            \n",
        "        # Reset Hidden State\n",
        "        # If we dont' reset we would backpropagate through all training history\n",
        "          hidden = tuple([state.data for state in hidden])\n",
        "        \n",
        "          model.zero_grad()\n",
        "        \n",
        "          lstm_output, hidden = model.forward(inputs,hidden)\n",
        "          loss = criterion(lstm_output,targets.view(batch_size*sequation_lenght,-1))\n",
        "        \n",
        "          loss.backward()\n",
        "        \n",
        "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
        "        # LET\"S CLIP JUST IN CASE\n",
        "          nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "        \n",
        "          optimizer.step()\n",
        "        \n",
        " #chechking on test_dataset      \n",
        "        if step % 25 == 0:\n",
        "            \n",
        "            val_hidden = model.hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            \n",
        "            for x,y in batch(test_dataset,batch_size,sequation_lenght):\n",
        "                \n",
        "                # One Hot Encode incoming data\n",
        "                x = transform(model,x)\n",
        "                y = transform(model,y)\n",
        "\n",
        "                # Convert Numpy Arrays to Tensor\n",
        "\n",
        "                inputs = torch.from_numpy(x).float()\n",
        "                targets = torch.from_numpy(y).float()\n",
        "\n",
        "                # Adjust for GPU if necessary\n",
        "\n",
        "                if model.use_gpu:\n",
        "\n",
        "                    inputs = inputs.cuda()\n",
        "                    targets = targets.cuda()\n",
        "                    \n",
        "                # Reset Hidden State\n",
        "                # If we dont' reset we would backpropagate through \n",
        "                # all training history\n",
        "                val_hidden = tuple([state.data for state in val_hidden])\n",
        "                \n",
        "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
        "                val_loss = criterion(lstm_output,targets.view(batch_size*sequation_lenght,-1))\n",
        "        \n",
        "                val_losses.append(val_loss.item())\n",
        "            \n",
        "            # Reset to training model after val for loop\n",
        "            model.train()\n",
        "            \n",
        "            print(f\"Epoch: {i} Step: {step} Val Loss: {val_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feeb288f",
      "metadata": {
        "id": "feeb288f"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),'new_music.net')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b45650",
      "metadata": {
        "id": "14b45650"
      },
      "outputs": [],
      "source": [
        "model = Model_(dataset=dataset, num_hidden=50, num_layers=2, drop_prob=0.5, use_gpu=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63653ca7",
      "metadata": {
        "id": "63653ca7",
        "outputId": "ea3d07a6-6c8e-4b80-f5df-aa4714cbeb32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "noteModel(\n",
              "  (lstm): LSTM(29, 50, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=50, out_features=29, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "noteModel = Model_\n",
        "a = 'new_music.net'\n",
        "model.load_state_dict(torch.load(a))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ffa754",
      "metadata": {
        "id": "40ffa754",
        "outputId": "06d38b8a-6e00-4cf8-c95d-6c401430c36d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[76.  ,  0.25],\n",
              "       [74.  ,  0.25],\n",
              "       [76.  ,  0.25],\n",
              "       [74.  ,  0.25],\n",
              "       [74.  ,  0.5 ],\n",
              "       [74.  ,  0.25],\n",
              "       [74.  ,  0.25],\n",
              "       [ 0.  ,  0.5 ],\n",
              "       [72.  ,  0.25],\n",
              "       [74.  ,  0.25]])"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#her bir değerin karşılığında random bir nota atadığının kanıtı\n",
        "\n",
        "output(model,dataset[115:125].values)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "lstm_music_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}